{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yeJGnCYxuF"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Predicción de próxima palabra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv5PEwGzZA9-"
      },
      "source": [
        "### Objetivo\n",
        "El objetivo es utilizar documentos / corpus para crear embeddings de palabras basado en ese contexto utilizando la layer Embedding de Keras. Se utilizará esos embeddings junto con layers LSTM para predeccir la próxima posible palabra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-QdFbHZYj7C"
      },
      "source": [
        "import random\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torchsummar actualmente tiene un problema con las LSTM, por eso\n",
        "# se utiliza torchinfo, un fork del proyecto original con el bug solucionado\n",
        "!pip3 install torchinfo\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzYQL2mV-_Rt",
        "outputId": "d8311c86-1407-4097-cbca-172c679b12d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import platform\n",
        "\n",
        "if os.access('torch_helpers.py', os.F_OK) is False:\n",
        "    if platform.system() == 'Windows':\n",
        "        !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py\n",
        "    else:\n",
        "        !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI7mSG6p_B0t",
        "outputId": "7eebbce9-54c1-4f6e-ec58-7c8707f8f2af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-22 23:09:01--  http://torch_helpers.py/\n",
            "Resolving torch_helpers.py (torch_helpers.py)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘torch_helpers.py’\n",
            "--2024-08-22 23:09:01--  https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23883 (23K) [text/plain]\n",
            "Saving to: ‘torch_helpers.py’\n",
            "\n",
            "torch_helpers.py    100%[===================>]  23.32K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-08-22 23:09:01 (14.5 MB/s) - ‘torch_helpers.py’ saved [23883/23883]\n",
            "\n",
            "FINISHED --2024-08-22 23:09:01--\n",
            "Total wall clock time: 0.3s\n",
            "Downloaded: 1 files, 23K in 0.002s (14.5 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_helpers import categorical_acc\n",
        "\n",
        "def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100):\n",
        "    # Defino listas para realizar graficas de los resultados\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    valid_loss = []\n",
        "    valid_accuracy = []\n",
        "\n",
        "    # Defino mi loop de entrenamiento\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_train_accuracy = 0.0\n",
        "\n",
        "        for train_data, train_target in train_loader:\n",
        "            # Seteo los gradientes en cero ya que, por defecto, PyTorch\n",
        "            # los va acumulando\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(train_data)\n",
        "\n",
        "            # Computo el error de la salida comparando contra las etiquetas\n",
        "            loss = criterion(output, train_target)\n",
        "\n",
        "            # Almaceno el error del batch para luego tener el error promedio de la epoca\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "            # Computo el nuevo set de gradientes a lo largo de toda la red\n",
        "            loss.backward()\n",
        "\n",
        "            # Realizo el paso de optimizacion actualizando los parametros de toda la red\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculo el accuracy del batch\n",
        "            accuracy = categorical_acc(output, train_target)\n",
        "            # Almaceno el accuracy del batch para luego tener el accuracy promedio de la epoca\n",
        "            epoch_train_accuracy += accuracy.item()\n",
        "\n",
        "        # Calculo la media de error para la epoca de entrenamiento.\n",
        "        # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca.\n",
        "        epoch_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_loss.append(epoch_train_loss)\n",
        "        epoch_train_accuracy = epoch_train_accuracy / len(train_loader)\n",
        "        train_accuracy.append(epoch_train_accuracy)\n",
        "\n",
        "        # Realizo el paso de validación computando error y accuracy, y\n",
        "        # almacenando los valores para imprimirlos y graficarlos\n",
        "        valid_data, valid_target = next(iter(valid_loader))\n",
        "        output = model(valid_data)\n",
        "\n",
        "        epoch_valid_loss = criterion(output, valid_target).item()\n",
        "        valid_loss.append(epoch_valid_loss)\n",
        "\n",
        "        # Calculo el accuracy de la epoch\n",
        "        epoch_valid_accuracy = categorical_acc(output, valid_target).item()\n",
        "        valid_accuracy.append(epoch_valid_accuracy)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Train accuracy {epoch_train_accuracy:.3f} - Valid Loss {epoch_valid_loss:.3f} - Valid accuracy {epoch_valid_accuracy:.3f}\")\n",
        "\n",
        "    history = {\n",
        "        \"loss\": train_loss,\n",
        "        \"accuracy\": train_accuracy,\n",
        "        \"val_loss\": valid_loss,\n",
        "        \"val_accuracy\": valid_accuracy,\n",
        "    }\n",
        "    return history"
      ],
      "metadata": {
        "id": "3QPapqTx_Lgt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvXlEKQZdqx"
      },
      "source": [
        "### Datos\n",
        "Utilizaremos como dataset canciones de bandas de habla inglés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkdPfrQJZdB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b940bcce-3e8a-4fae-9f66-9084d79e7546"
      },
      "source": [
        "# Descargar la carpeta de dataset\n",
        "import os\n",
        "import platform\n",
        "if os.access('./songs_dataset', os.F_OK) is False:\n",
        "    if os.access('songs_dataset.zip', os.F_OK) is False:\n",
        "        if platform.system() == 'Windows':\n",
        "            !curl https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip -o songs_dataset.zip\n",
        "        else:\n",
        "            !wget songs_dataset.zip https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/datasets/songs_dataset.zip\n",
        "    !unzip -q songs_dataset.zip\n",
        "else:\n",
        "    print(\"El dataset ya se encuentra descargado\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-22 23:09:01--  http://songs_dataset.zip/\n",
            "Resolving songs_dataset.zip (songs_dataset.zip)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘songs_dataset.zip’\n",
            "--2024-08-22 23:09:01--  https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/datasets/songs_dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip [following]\n",
            "--2024-08-22 23:09:02--  https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2075036 (2.0M) [application/zip]\n",
            "Saving to: ‘songs_dataset.zip’\n",
            "\n",
            "songs_dataset.zip   100%[===================>]   1.98M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-08-22 23:09:02 (49.9 MB/s) - ‘songs_dataset.zip’ saved [2075036/2075036]\n",
            "\n",
            "FINISHED --2024-08-22 23:09:02--\n",
            "Total wall clock time: 0.9s\n",
            "Downloaded: 1 files, 2.0M in 0.04s (49.9 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j-3nQ4lZjfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84cb3ee1-2153-4a0d-ef7a-dc634d639908"
      },
      "source": [
        "# Posibles bandas\n",
        "os.listdir(\"./songs_dataset/\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bob-marley.txt',\n",
              " 'prince.txt',\n",
              " 'jimi-hendrix.txt',\n",
              " 'blink-182.txt',\n",
              " 'notorious_big.txt',\n",
              " 'nickelback.txt',\n",
              " 'adele.txt',\n",
              " 'nirvana.txt',\n",
              " 'paul-simon.txt',\n",
              " 'cake.txt',\n",
              " 'kanye.txt',\n",
              " 'michael-jackson.txt',\n",
              " 'johnny-cash.txt',\n",
              " 'dr-seuss.txt',\n",
              " 'bjork.txt',\n",
              " 'lil-wayne.txt',\n",
              " 'bruno-mars.txt',\n",
              " 'bruce-springsteen.txt',\n",
              " 'dickinson.txt',\n",
              " 'bieber.txt',\n",
              " 'lady-gaga.txt',\n",
              " 'Kanye_West.txt',\n",
              " 'nicki-minaj.txt',\n",
              " 'kanye-west.txt',\n",
              " 'joni-mitchell.txt',\n",
              " 'britney-spears.txt',\n",
              " 'r-kelly.txt',\n",
              " 'leonard-cohen.txt',\n",
              " 'disney.txt',\n",
              " 'notorious-big.txt',\n",
              " 'alicia-keys.txt',\n",
              " 'lorde.txt',\n",
              " 'missy-elliott.txt',\n",
              " 'al-green.txt',\n",
              " 'patti-smith.txt',\n",
              " 'amy-winehouse.txt',\n",
              " 'rihanna.txt',\n",
              " 'drake.txt',\n",
              " 'Lil_Wayne.txt',\n",
              " 'radiohead.txt',\n",
              " 'ludacris.txt',\n",
              " 'dj-khaled.txt',\n",
              " 'dolly-parton.txt',\n",
              " 'janisjoplin.txt',\n",
              " 'beatles.txt',\n",
              " 'eminem.txt',\n",
              " 'lin-manuel-miranda.txt',\n",
              " 'bob-dylan.txt',\n",
              " 'nursery_rhymes.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb39v3PaZmRH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "cc72979b-f6a7-4c6c-ce68-7d49698cf3b7"
      },
      "source": [
        "# Armar el dataset utilizando salto de línea para separar las oraciones/docs\n",
        "df = pd.read_csv('songs_dataset/beatles.txt', sep='/n', header=None)\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-91c4bad52ba1>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  df = pd.read_csv('songs_dataset/beatles.txt', sep='/n', header=None)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   0\n",
              "0      Yesterday, all my troubles seemed so far away\n",
              "1        Now it looks as though they're here to stay\n",
              "2  Oh, I believe in yesterday Suddenly, I'm not h...\n",
              "3                  There's a shadow hanging over me.\n",
              "4  Oh, yesterday came suddenly Why she had to go ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7da32e32-1478-48e0-ad8b-bb6bf0b1b1ad\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Yesterday, all my troubles seemed so far away</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Now it looks as though they're here to stay</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Oh, I believe in yesterday Suddenly, I'm not h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>There's a shadow hanging over me.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Oh, yesterday came suddenly Why she had to go ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7da32e32-1478-48e0-ad8b-bb6bf0b1b1ad')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7da32e32-1478-48e0-ad8b-bb6bf0b1b1ad button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7da32e32-1478-48e0-ad8b-bb6bf0b1b1ad');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d60fbe4c-c807-4fce-95db-270a85fb4570\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d60fbe4c-c807-4fce-95db-270a85fb4570')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d60fbe4c-c807-4fce-95db-270a85fb4570 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1846,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1380,\n        \"samples\": [\n          \"I just need someone to love\",\n          \"Well it's my birthday too--yeah\",\n          \"Yes, you can radiate everything you are\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riT898QlZnmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "271b4461-d1a3-4d01-dd18-89fd39dc095c"
      },
      "source": [
        "print(\"Cantidad de documentos:\", df.shape[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de documentos: 1846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDoouHp7Zp6D"
      },
      "source": [
        "### 1 - Ejemplo de Preprocesamiento\n",
        "- Hay que transformar las oraciones en tokens.\n",
        "- Dichas oraciones hay que ajustarlas al tamaño fijo de nuestra sentencia de entrada al modelo.\n",
        "- Hay que separar las palabras objetivos (target) que el modelo debe predecir en cada sentencia armada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5FeTaGvbDbw"
      },
      "source": [
        "from torch_helpers import Tokenizer # tool de keras equivalente a ltokenizer de nltk\n",
        "from torch_helpers import text_to_word_sequence # tool de keras equivalente a word_teokenize de nltk\n",
        "from torch_helpers import pad_sequences # tool de keras qye se utilizará para padding\n",
        "\n",
        "# largo de la secuencia, incluye seq input + word output\n",
        "train_len = 4"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kMIpWjr-xmxX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf3O7eK6ZpP8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "25d2fb42-325e-4400-a92a-e34b3dc0abec"
      },
      "source": [
        "# Ejemplo de como transformar una oración a tokens usando keras\n",
        "text = df.loc[0,0]\n",
        "text"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yesterday, all my troubles seemed so far away'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOv67Sj7aeFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd29030c-a8fd-46e5-9d58-932baaa85ead"
      },
      "source": [
        "tokens = text_to_word_sequence(text) # entran oraciones -> salen vectores de N posiciones (tokens)\n",
        "tokens"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yesterday', 'all', 'my', 'troubles', 'seemed', 'so', 'far', 'away']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrlyqkoiaymK"
      },
      "source": [
        "1.1 - Transformar las oraciones en secuencias (tokens) de palabras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH_L14Wjaowe"
      },
      "source": [
        "# Recorrer todas las filas y transformar las oraciones\n",
        "# en secuencias de palabras\n",
        "sentence_tokens = []\n",
        "for _, row in df[:None].iterrows():\n",
        "    sentence_tokens.append(text_to_word_sequence(row[0]))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KASzU4CdaxbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9feb934b-8317-4720-dab0-9f253c6f9d91"
      },
      "source": [
        "# Demos un vistazo\n",
        "sentence_tokens[:2]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['yesterday', 'all', 'my', 'troubles', 'seemed', 'so', 'far', 'away'],\n",
              " ['now', 'it', 'looks', 'as', 'though', \"they're\", 'here', 'to', 'stay']]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A659lswTbIIB"
      },
      "source": [
        "# Código para hacer el desfazaje de las palabras\n",
        "# según el train_len\n",
        "text_sequences = []\n",
        "\n",
        "for i in range(train_len, len(tokens)):\n",
        "  seq = tokens[i-train_len:i]\n",
        "  text_sequences.append(seq)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01JEoPPnbgRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "556acecb-1c96-4127-ed2c-339c2d817da0"
      },
      "source": [
        "# Demos un vistazo a nuestros vectores para entrenar el modelo\n",
        "# seq_input + output\n",
        "text_sequences"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['yesterday', 'all', 'my', 'troubles'],\n",
              " ['all', 'my', 'troubles', 'seemed'],\n",
              " ['my', 'troubles', 'seemed', 'so'],\n",
              " ['troubles', 'seemed', 'so', 'far']]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B0gHnKVa4W_"
      },
      "source": [
        "1.2 - Crear los vectores de palabras (word2vec)\n",
        "\n",
        "Ahora necesitamos pasarlos a números para que lo entienda la red y separar input de output.\n",
        "- El Input seran integers (word2vec)\n",
        "- Mientras que el output será one hot encodeado (labels) del tamaño del vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkPNvXeQcS0U"
      },
      "source": [
        "tok = Tokenizer()\n",
        "\n",
        "# El tokeinzer \"aprende\" las palabras que se usaran\n",
        "# Se construye (fit) una vez por proyecto, se aplica N veces (tal cual un encoder)\n",
        "tok.fit_on_texts(text_sequences)\n",
        "\n",
        "# Convertimos las palabras a números\n",
        "# entran palabras -> salen números\n",
        "sequences = tok.texts_to_sequences(text_sequences)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SIc44IocyQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13f4196-0a97-4c65-ee3c-dd21d4abe283"
      },
      "source": [
        "# Ahora sequences tiene los números \"ID\", largo 4\n",
        "sequences"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[6, 4, 2, 1], [4, 2, 1, 3], [2, 1, 3, 5], [1, 3, 5, 7]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ro81yCQc1oX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf76f46c-2d8a-49cc-ed84-c87bdb0f529a"
      },
      "source": [
        "# Cantidad de casos (doc) de entrada\n",
        "print(tok.document_count)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzAWNfroc4u1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef45897-32e9-46d6-fdb2-402e0e1b4651"
      },
      "source": [
        "# Cantidad de palabras distintas\n",
        "print(len(tok.word_counts))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spTBxmFQc6h8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f3e38d-208c-4ba3-b9b6-465e98577896"
      },
      "source": [
        "# El índice para cada palabra\n",
        "# El sistema las ordena de las más populares a las menos populares\n",
        "print(tok.word_index)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'troubles': 1, 'my': 2, 'seemed': 3, 'all': 4, 'so': 5, 'yesterday': 6, 'far': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUDkjy80c77h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddca29f6-4bc5-4d05-a513-6646fcd912f5"
      },
      "source": [
        "# Cantidad de veces quea aparece cada palabra en cada \"documento\"\n",
        "# (1 documento = 1 caso de entrada)\n",
        "print(tok.word_docs)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {'troubles': 4, 'yesterday': 1, 'my': 3, 'all': 2, 'seemed': 3, 'so': 2, 'far': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohS5Tao1d2KB"
      },
      "source": [
        "### 2 - Preprocesamiento completo\n",
        "Debemos realizar los mismos pasos que en el ejemplo anterior, pero antes de eso debemos transformar ese dataset de filas de oraciones en un texto completo continuo para poder extraer el vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Z2-Se2t27r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "6c7a4109-9a77-4ddd-8ce5-9a9ee7a11528"
      },
      "source": [
        "# Vistazo a las primeras filas\n",
        "df.loc[:15,0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         Yesterday, all my troubles seemed so far away\n",
              "1           Now it looks as though they're here to stay\n",
              "2     Oh, I believe in yesterday Suddenly, I'm not h...\n",
              "3                     There's a shadow hanging over me.\n",
              "4     Oh, yesterday came suddenly Why she had to go ...\n",
              "5     I said something wrong, now I long for yesterd...\n",
              "6                       Now I need a place to hide away\n",
              "7     Oh, I believe in yesterday Why she had to go I...\n",
              "8     I said something wrong, now I long for yesterd...\n",
              "9                       Now I need a place to hide away\n",
              "10                           Oh, I believe in yesterday\n",
              "11    Mm mm mm mm mm mm mm When I find myself in tim...\n",
              "12                  Speaking words of wisdom, let it be\n",
              "13    And in my hour of darkness she is standing rig...\n",
              "14                  Speaking words of wisdom, let it be\n",
              "15           Let it be, let it be, let it be, let it be\n",
              "Name: 0, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Yesterday, all my troubles seemed so far away</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Now it looks as though they're here to stay</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Oh, I believe in yesterday Suddenly, I'm not h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>There's a shadow hanging over me.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Oh, yesterday came suddenly Why she had to go ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I said something wrong, now I long for yesterd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Now I need a place to hide away</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Oh, I believe in yesterday Why she had to go I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>I said something wrong, now I long for yesterd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Now I need a place to hide away</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Oh, I believe in yesterday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Mm mm mm mm mm mm mm When I find myself in tim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Speaking words of wisdom, let it be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>And in my hour of darkness she is standing rig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Speaking words of wisdom, let it be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Let it be, let it be, let it be, let it be</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kILsSoxTuHEr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "56550946-031c-47f0-a81b-99689a84b50a"
      },
      "source": [
        "# Concatenamos todos los rows en un solo valor\n",
        "corpus = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=0)[0]\n",
        "corpus"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yesterday, all my troubles seemed so far away Now it looks as though they\\'re here to stay Oh, I believe in yesterday Suddenly, I\\'m not half the man I used to be There\\'s a shadow hanging over me. Oh, yesterday came suddenly Why she had to go I don\\'t know she wouldn\\'t say I said something wrong, now I long for yesterday Yesterday, love was such an easy game to play Now I need a place to hide away Oh, I believe in yesterday Why she had to go I don\\'t know she wouldn\\'t say I said something wrong, now I long for yesterday Yesterday, love was such an easy game to play Now I need a place to hide away Oh, I believe in yesterday Mm mm mm mm mm mm mm When I find myself in times of trouble, Mother Mary comes to me Speaking words of wisdom, let it be And in my hour of darkness she is standing right in front of me Speaking words of wisdom, let it be Let it be, let it be, let it be, let it be Whisper words of wisdom, let it be And when the broken hearted people living in the world agree There will be an answer, let it be For though they may be parted, there is still a chance that they will see There will be an answer, let it be Let it be, let it be, let it be, let it be There will be an answer, let it be Let it be, let it be, let it be, let it be Whisper words of wisdom, let it be Let it be, let it be, let it be, let it be Whisper words of wisdom, let it be And when the night is cloudy there is still a light that shines on me Shine until tomorrow, let it be I wake up to the sound of music, Mother Mary comes to me Speaking words of wisdom, let it be Let it be, let it be, let it be, yeah, let it be There will be an answer, let it be Let it be, let it be, let it be, yeah, let it be Whisper words of wisdom, let it be Words are flowing out like Endless rain into a paper cup They slither wildly as they slip away across the universe. Pools of sorrow waves of joy Are drifting through my opened mind Possessing and caressing me. Jai Guru Deva. Om Nothing\\'s gonna change my world Nothing\\'s gonna change my world Nothing\\'s gonna change my world Nothing\\'s gonna change my world Images of broken light, which Dance before me like a million eyes, They call me on and on across the universe. Thoughts meander like a Restless wind inside a letter box They tumble blindly as they make their way across the universe. Jai Guru Deva. Om Nothing\\'s gonna change my world Nothing\\'s gonna change my world Nothing\\'s gonna change my world Nothing\\'s gonna change my world Sounds of laughter, shades of life Are ringing through my opened ears Inciting and inviting me. Limitless undying love, which Shines around me like a million suns, It calls me on and on across the universe Jai Guru Deva, om. Nothing\\'s gonna change my world Nothing\\'s gonna change my world Nothing\\'s gonna change my world Nothing\\'s gonna change my world Jai Guru Deva Jai Guru Deva Jai Guru Deva Jai Guru Deva Jai Guru Deva Hey Jude, don\\'t make it bad Take a sad song and make it better Remember to let her into your heart Then you can start to make it better Hey Jude, don\\'t be afraid You were made to go out and get her The minute you let her under your skin Then you begin to make it better And anytime you feel the pain, hey Jude, refrain Don\\'t carry the world upon your shoulders For well you know that it\\'s a fool who plays it cool By making his world a little colder Nah nah nah nah nah nah nah nah nah Hey Jude, don\\'t let me down You have found her, now go and get her Remember to let her into your heart Then you can start to make it better So let it out and let it in, hey Jude, begin You\\'re waiting for someone to perform with And don\\'t you know that it\\'s just you, hey Jude, you\\'ll do The movement you need is on your shoulder Nah nah nah nah nah nah nah nah nah yeah Hey Jude, don\\'t make it bad Take a sad song and make it better Remember to let her under your skin Then you\\'ll begin to make it Better better better better better better, oh Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude Nah nah nah nah nah nah, nah nah nah, hey Jude There are places I\\'ll remember All my life, though some have changed Some forever, not for better Some have gone and some remain All these places have their moments With lovers and friends I still can recall Some are dead and some are living In my life, I\\'ve loved them all But of all these friends and lovers There is no one compares with you And these memories lose their meaning When I think of love as something new Though I know I\\'ll never lose affection For people and things that went before I know I\\'ll often stop and think about them In my life, I love you more Though I know I\\'ll never lose affection For people and things that went before I know I\\'ll often stop and think about them In my life, I love you more In my life-- I love you more Blackbird singing in the dead of night Take these broken wings and learn to fly All your life You were only waiting for this moment to arise Blackbird singing in the dead of night Take these sunken eyes and learn to see All your life You were only waiting for this moment to be free Blackbird fly, blackbird fly Into the light of the dark black night Blackbird fly, blackbird fly Into the light of the dark black night Blackbird singing in the dead of night Take these broken wings and learn to fly All your life You were only waiting for this moment to arise You were only waiting for this moment to arise You were only waiting for this moment to arise Here comes the sun (doo doo doo doo) Here comes the sun, and I say It\\'s all right Little darling, it\\'s been a long cold lonely winter Little darling, it feels like years since it\\'s been here Here comes the sun Here comes the sun, and I say It\\'s all right Little darling, the smiles returning to the faces Little darling, it seems like years since it\\'s been here Here comes the sun Here comes the sun, and I say It\\'s all right Sun, sun, sun, here it comes Sun, sun, sun, here it comes Sun, sun, sun, here it comes Sun, sun, sun, here it comes Sun, sun, sun, here it comes Little darling, I feel that ice is slowly melting Little darling, it seems like years since it\\'s been clear Here comes the sun Here comes the sun, and I say It\\'s all right Here comes the sun Here comes the sun, and I say It\\'s all right It\\'s all right Once there was a way, to get back homeward, Once there was a way, to get back home Sleep pretty darling do not cry, and I will sing a lullaby Golden slumbers fill your eyes, smiles awake you when you rise Sleep pretty darling do not cry, and I will sing a lullaby Once there was a way, to get back homeward, Once there was a way, to get back home Sleep pretty darling do not cry, and I will sing a lullaby Boy you\\'re gonna carry that weight, carry that weight for a long time Boy you\\'re gonna carry that weight, carry that weight for a long time I never give you my pillow, I only send you my invitations And in the middle of the celebrations, I break down Boy you\\'re gonna carry that weight, carry that weight for a long time Boy you\\'re gonna carry that weight, carry that weight for a long time Oh yeah, all right, are you gonna be in my dreams tonight? Love you, love you, love you, love you, love you, love you Love you, love you, love you, love you, love you, love you Love you, love you, love you, love you, love you, love you Love you, love you, love you, love you, love you, love you Love you, love you, love you, love you, love you, love you And in the end, the love you take, is equal to the love you make, Ah Love, love, love, love, love, love, love, love, love. There\\'s nothing you can do that can\\'t be done. Nothing you can sing that can\\'t be sung. Nothing you can say, but you can learn How to play the game It\\'s easy. Nothing you can make that can\\'t be made. No one you can save that can\\'t be saved. Nothing you can do, but you can learn How to be you in time It\\'s easy. All you need is love, all you need is love, All you need is love, love. Love is all you need. Love, love, love, love, love, love, love, love, love. All you need is love, all you need is love, All you need is love, love. Love is all you need. There\\'s nothing you can know that isn\\'t known. Nothing you can see that isn\\'t shown. There\\'s nowhere you can be that isn\\'t where You\\'re meant to be It\\'s easy. All you need is love, all you need is love, All you need is love, love. Love is all you need. All you need is love. (All together now). All you need is love. (Everybody). All you need is love, love. Love is all you need. Love is all you need. Love is all you need (Yesterday) (Oh yeah) (She love you, yeah, yeah, yeah) (She love you, yeah, yeah, yeah) (Oh, yesterday) There are places I remember all my life Though some have changed Some forever, not for better Some have gone and some remain All these places have their moments Of lovers and friends I still can recall Some are dead and some are living In my life I loved them all And with all these friends and lovers There is no one compares with you And these mem\\'ries lose their meaning When I think of love as something new And I know I\\'ll never lose affection For people and things that went before I know I\\'ll often stop and think about them In my life I loved you more And I know I\\'ll never lose affection For people and things that went before I know I\\'ll often stop and think about them In my life I loved you more In my life I loved you more Let me take you down \\'Cause I\\'m going to Strawberry Fields Nothing is real And nothing to get hung about Strawberry Fields forever Living is easy with eyes closed Misunderstanding all you see It\\'s getting hard to be someone But it all works out It doesn\\'t matter much to me Let me take you down \\'Cause I\\'m going to Strawberry Fields Nothing is real And nothing to get hung about Strawberry Fields forever No one I think is in my tree I mean it must be high or low That is you know you can\\'t tune it But it\\'s all right That is I think it\\'s not too bad Let me take you down \\'Cause I\\'m going to Strawberry Fields Nothing is real And nothing to get hung about Strawberry Fields forever Always know sometimes it\\'s me But you know I know when it\\'s a dream I think I know I mean a \"Yes\" But it\\'s all wrong That is I think I disagree Let me take you down \\'Cause I\\'m going to Strawberry Fields Nothing is real And nothing to get hung about Strawberry Fields forever Strawberry Fields forever Strawberry Fields forever Desmond has a barrow in the marketplace Molly is the singer in a band Desmond says to Molly girl I like your face And Molly says this as she takes him by the hand [Chorus] Ob la di ob la da life goes on bra La la how the life goes on Ob la di ob la da life goes on bra La la how the life goes on Desmond takes a trolley to the jeweler\\'s store Buys a twenty carat golden ring Takes it back to Molly waiting at the door And as he gives it to her she begins to sing [Chorus] In a couple of years they have built A home sweet home With a couple of kids running in the yard Of Desmond and Molly Jones Happy ever after in the market place Desmond lets the children lend a hand Molly stays at home and does her pretty face And in the evening she still sings it with the band [Chorus] In a couple of years they have built A home sweet home With a couple of kids running in the yard Of Desmond and Molly Jones Happy ever after in the market place Molly lets the children lend a hand Desmond stays at home and does his pretty face And in the evening he\\'s a singer with the band [Chorus] And if you want some fun sing ob la di bla da When I get older, losing my hair, many years from now Will you still be sending me a valentine, birthday greetings, bottle of wine? If I\\'d been out \\'til quarter to three, would you lock the door? Will you still need me, will you still feed me, when I\\'m sixty-four? You\\'ll be older too Ah And, if you say the word, I could stay with you I could be handy, mending a fuse, when your lights have gone You can knit a sweater by the fireside, Sunday mornings, go for a ride Doing the garden, digging the weeds, who could ask for more? Will you still need me, will you still feed me, when I\\'m sixty-four? Every summer we can rent a cottage In the Isle of Wight if it\\'s not too dear We shall scrimp and save Ah Grandchildren on your knee Vera, Chuck, and Dave Send me a postcard, drop me a line, stating point of view Indicate precisely what you mean to say, yours sincerely, wasting away Give me your answer, fill in a form, mine forever more Will you still need me, will you still feed me when I\\'m sixty-four? Oh yeah I tell you somethin\\' I think you\\'ll understand When I say that somethin\\' I want to hold your hand I want to hold your hand I want to hold your hand Oh please say to me You\\'ll let me be your man And please say to me You\\'ll let me hold your hand Now, let me hold your hand I want to hold your hand And when I touch you I feel happy inside It\\'s such a feelin\\' that my love I can\\'t hide I can\\'t hide I can\\'t hide Yeah, you got that somethin\\' I think you\\'ll understand When I say that somethin\\' I want to hold your hand I want to hold your hand I want to hold your hand And when I touch you I feel happy inside It\\'s such a feelin\\' that my love I can\\'t hide I can\\'t hide I can\\'t hide Yeah, you got that somethin\\' I think you\\'ll understand When I feel that somethin\\' I want to hold your hand I want to hold your hand I want to hold your hand I want to hold your hand Don\\'t let me down, don\\'t let me down Don\\'t let me down, don\\'t let me down Nobody ever loved me like she does Oh, she does, yeah, she does And if somebody loved me like she do me Oh, she do me, yes, she does Don\\'t let me down, don\\'t let me down Don\\'t let me down, don\\'t let me down I\\'m in love for the first time Don\\'t you know it\\'s gonna last It\\'s a love that lasts forever It\\'s a love that had no past Don\\'t let me down, don\\'t let me down Don\\'t let me down, don\\'t let me down And from the first time that she really done me Oh, she done me, she done me good I guess nobody ever really done me Oh, she done me, she done me good Don\\'t let me down, don\\'t let me down Don\\'t let me down Help, I need somebody Help, not just anybody Help, you know I need someone, help When I was younger, so much younger than today I never needed anybody\\'s help in any way But now these days are gone, I\\'m not so self assured Now I find I\\'ve changed my mind and opened up the doors Help me if you can, I\\'m feeling down And I do appreciate you being round Help me, get my feet back on the ground Won\\'t you please, please help me And now my life has changed in oh so many ways My independence seems to vanish in the haze But every now and then I feel so insecure I know that I just need you like I\\'ve never done before Help me if you can, I\\'m feeling down And I do appreciate you being round Help me, get my feet back on the ground Won\\'t you please, please help me When I was younger, so much younger than today I never needed anybody\\'s help in any way But now these days are gone, I\\'m not so self assured Now I find I\\'ve changed my mind and opened up the doors Help me if you can, I\\'m feeling down And I do appreciate you being round Help me, get my feet back on the ground Won\\'t you please, please help me, help me, help me, oh Something in the way she moves Attracts me like no other lover Something in the way she woos me I don\\'t want to leave her now You know I believe and how Somewhere in her smile she knows That I don\\'t need no other lover Something in her style that shows me I don\\'t want to leave her now You know I believe and how You\\'re asking me will my love grow I don\\'t know, I don\\'t know You stick around and it may show I don\\'t know, I don\\'t know Something in the way she knows And all I have to do is think of her Something in the things she shows me I don\\'t want to leave her now You know I believe and how Here come old flat top He come groovin\\' up slowly He got joo joo eyeballs He one holy roller He got hair down to his knee Got to be a joker He just do what he please He wear no shoeshine He got toe jam football He got monkey finger He shoot Coca-Cola He say I know you, you know me One thing I can tell you is You got to be free Come together, right now Over me He bad production He got walrus gumboot He got Ono sideboard He one spinal cracker He got feet down below his knee Hold you in his armchair You can feel his disease Come together, right now Over me He roller coaster He got early warning He got muddy water He one Mojo filter He say one and one and one is three Got to be good looking \\'Cause he\\'s so hard to see Come together right now Over me Come together, yeah Come together, yeah Come together, yeah Come together, yeah Come together, yeah Come together, yeah Come together, yeah Come together, yeah Come together, yeah There were bells on a hill But I never heard them ringing No, I never heard them at all Till there was you There were birds in the sky But I never saw them winging No, I never saw them at all Till there was you Then there was music and wonderful roses They tell me in sweet fragrant meadows Of dawn and dew There was love all around But I never heard it singing No I never heard it at all Till there was you Then there was music and wonderful roses They tell me in sweet fragrant meadows Of dawn and dew There was love all around But I never heard it singing No, I never heard it at all Till there was you Till there was you You say you want a revolution Well, you know We all want to change the world You tell me that it\\'s evolution Well, you know We all want to change the world But when you talk about destruction Don\\'t you know that you can count me out Don\\'t you know it\\'s gonna be All right, all right, all right You say you got a real solution Well, you know We\\'d all love to see the plan You ask me for a contribution Well, you know We\\'re doing what we can But if you want money for people with minds that hate All I can tell is brother you have to wait Don\\'t you know it\\'s gonna be All right, all right, all right You say you\\'ll change the constitution Well, you know We all want to change your head You tell me it\\'s the institution Well, you know You better free you mind instead But if you go carrying pictures of chairman Mao You ain\\'t going to make it with anyone anyhow Don\\'t you know it\\'s gonna be All right, all right, all right All right, all right, all right All right, all right, all right All right, all right In the town where I was born Lived a man who sailed to sea And he told us of his life In the land of submarines So we sailed up to the sun Till we found a sea of green And we lived beneath the waves In our yellow submarine We all live in a yellow submarine Yellow submarine, yellow submarine We all live in a yellow submarine Yellow submarine, yellow submarine And our friends are all aboard Many more of them live next door And the band begins to play We all live in a yellow submarine Yellow submarine, yellow submarine We all live in a yellow submarine Yellow submarine, yellow submarine (Full speed ahead Mr. Boatswain, full speed ahead Full speed ahead it is, Sergeant. Cut the cable, drop the cable Aye, Sir, aye Captain, captain) As we live a life of ease Every one of us has all we need Sky of blue and sea of green In our yellow submarine We all live in a yellow submarine Yellow submarine, yellow submarine We all live in a yellow submarine Yellow submarine, yellow submarine We all live in a yellow submarine Yellow submarine, yellow submarine Who knows how long I\\'ve loved you You know I love you still Will I wait a lonely lifetime? If you want me to, I will And if I ever saw you I didn\\'t catch your name But it never really mattered I will always feel the same Love you forever and forever Love you with all my heart Love you whenever we\\'re together Love you when we\\'re apart And when at last I find you Your song will fill the air Sing it loud so I can hear you Make it easy to be near you For the things you do endear you to me Oh, you know I will Love you forever and forever Love you with all my heart Love you whenever we\\'re together Love you when we\\'re apart And when at last I found you Your song will fill the air Sing it loud so I can hear you Make it easy to be near you All the things you do endear you to me Oh, you know I will Oh, you know I will Oh, you know I will Oh, you know I will What would you think if I sang out of tune Would you stand up and walk out on me Lend me your ears and I\\'ll sing you a song And I\\'ll try not to sing out of key Oh, I get by with a little help from my friends Hmm, I get high with a little help from my friends Hmm, going to try with a little help from my friends What do I do when my love is away Does it worry you to be alone? How do I feel by the end of the day Are you sad because you\\'re on your own? No, I get by with a little help from my friends Hmm, I get high with a little help from my friends Hmm, going to try with a little help from my friends Do you need anybody? I need somebody to love Could it be anybody? I want somebody to love Would you believe in a love at first sight? Yes, I\\'m certain that it happens all the time What do you see when you turn out the light? I can\\'t tell you, but I know it\\'s mine Oh, I get by with a little help from my friends Hmm, I get high with a little help from my friends Hmm, going to try with a little help from my friends Do you need anybody? I just need someone to love Could it be anybody? I want somebody to love Oh, I get by with a little help from my friends Hmm, going to try with a little help from my friends Oh, I get high with a little help from my friends Yes, I get by with a little help from my friends With a little help from my friends I am he as you are he as you are me And we are all together See how they run like pigs from a gun See how they fly I\\'m crying Sitting on a cornflake Waiting for the van to come Corporation T-shirt, stupid bloody Tuesday Man you\\'ve been a naughty boy You let your face grow long I am the eggman They are the eggmen I am the walrus Goo goo g\\' joob Mr. City policeman sitting Pretty little policemen in a row See how they fly like Lucy in the sky See how they run I\\'m crying I\\'m crying, I\\'m crying, I\\'m crying Yellow matter custard Dripping from a dead dog\\'s eye Crabalocker fishwife Pornographic priestess Boy, you\\'ve been a naughty girl You let your knickers down I am the eggman They are the eggmen I am the walrus Goo goo g\\' joob Sitting in an English garden Waiting for the sun If the sun don\\'t come you get a tan From standing in the English rain I am the eggman (\"How do you do sir\") They are the eggmen (\"The man maintains a fortune\") I am the walrus Goo goo g\\' joob Goo Goo Goo g\\' joob Expert, texpert choking smokers Don\\'t you think the joker laughs at you (Ho ho ho hee hee hee hah hah hah) See how they smile like pigs in a sty See how they snide I\\'m crying Semolina Pilchard Climbing up the Eiffel tower Elementary penguin singing Hare Krishna Man, you should have seen them kicking Edgar Allen Poe I am the eggman They are the eggmen I am the walrus Goo goo g\\' joob Goo goo goo g\\' joob Goo goo g\\' joob Goo goo goo g\\' joob Goo goo Juba juba juba Juba juba juba Juba juba juba Juba juba (Oh I\\'m tired, servicible villain Set you down father, rest you) I look at you all see the love there that\\'s sleeping While my guitar gently weeps I look at the floor and I see it needs sweeping Still my guitar gently weeps. I don\\'t know why nobody told you How to unfold your love I don\\'t know how someone controlled you They bought and sold you. I look at the world and I notice it\\'s turning While my guitar gently weeps With every mistake we must surely be learning Still my guitar gently weeps. I don\\'t know how you were diverted You were perverted too I don\\'t know how you were inverted No one alerted you. I look at you all see the love there that\\'s sleeping While my guitar gently weeps Look at you all Still my guitar gently weeps. Oh yeah, all right Are you gonna be in my dreams tonight? Love you, love you Love you, love you Love you, love you Love you, love you Love you, love you Love you, love you Love you, love you Love you, love you Love you, love you Love you, love you Love you, love you Love you, love you And in the end, the love you take Is equal to the love you make I read the news today, oh boy About a lucky man who made the grade And though the news was rather sad Well I just had to laugh I saw the photograph He blew his mind out in a car He didn\\'t notice that the lights had changed A crowd of people stood and stared They\\'d seen his face before Nobody was really sure If he was from the House of Lords I saw a film today, oh boy The English Army had just won the war A crowd of people turned away But I just had to look Having read the book I\\'d love to turn you on Woke up, fell out of bed Dragged a comb across my head Found my way downstairs and drank a cup And looking up I noticed I was late Found my coat and grabbed my hat Made the bus in seconds flat Found my way upstairs and had a smoke Somebody spoke and I went into a dream I read the news today, oh boy Four thousand holes in Blackburn, Lancashire And though the holes were rather small They had to count them all Now they know how many holes it takes to fill the Albert Hall I\\'d love to turn you on Michelle, ma belle These are words that go together well My Michelle Michelle, ma belle Sont les mots qui vont tres bien ensemble Tres bien ensemble I love you, I love you, I love you That\\'s all I want to say Until I find a way I will say the only words I know that you\\'ll understand Michelle, ma belle Sont les mots qui vont tres bien ensemble Tres bien ensemble I need to, I need to, I need to I need to make you see Oh, what you mean to me Until I do, I\\'m hoping you will know what I mean I love you... I want you, I want you, I want you I think you know by now I\\'ll get to you somehow Until I do, I\\'m telling you so you\\'ll understand Michelle, ma belle Sont les mots qui vont tres bien ensemble Tres bien ensemble And I will say the only words I know that you\\'ll understand My Michelle Penny Lane there is a barber showing photographs Of every head he\\'s had the pleasure to know. And all the people that come and go Stop and say \"Hello\". On the corner is a banker with a motorcar, And little children laugh at him behind his back. And the banker never wears a mac In the pouring rain, very strange. Penny Lane is in my ears and in my eyes. There beneath the blue suburban skies I sit, and meanwhile back In Penny Lane there is a fireman with an hourglass, And in his pocket is a portrait of the Queen. He likes to keep his fire engine clean, It\\'s a clean machine. Penny Lane is in my ears and in my eyes. A four of fish and finger pies In summer. Meanwhile back Behind the shelter in the middle of the roundabout The pretty nurse is selling poppies from a tray. And though she feels as if she\\'s in a play, She is anyway. In Penny Lane the barber shaves another customer, We see the banker sitting waiting for a trim, And then the fireman rushes in From the pouring rain - very strange. Penny Lane is in my ears and in my eyes. There beneath the blue suburban skies I sit, and meanwhile back. Penny Lane is in my ears and in my eyes. There beneath the blue suburban skies Penny Lane! Jojo was a man who thought he was a loner But he knew it wouldn\\'t last Jojo left his home in Tucson, Arizona For some California grass Get back, get back Get back to where you once belonged Get back, get back Get back to where you once belonged Get back Jojo, go home Get back, get back Back to where you once belonged Get back, get back Back to where you once belonged Get back Jo Sweet Loretta Martin thought she was a woman But she was another man All the girls around her say she\\'s got it coming But she gets it while she can Get back, get back Get back to where you once belonged Get back, get back Get back to where you once belonged Get back Loretta, go home Get back, get back Get back to where you once belonged Get back, get back Get back to where you once belonged Get back, get back Get back to where you once belonged Get back, get back, get back Once there was a way, To get back homeward. Once there was a way To get back home. Sleep, pretty darling, Dot not cry And I will sing a lullaby. Golden slumbers, Fill your eyes Smiles await you when you rise Sleep pretty darling Do not cry And I will sing a lullaby. Once there was a way To get back homeward Once there was a way To get back home Sleep, pretty darling Do not cry And I will sing a lullaby. You say it\\'s your birthday It\\'s my birthday too--yeah They say it\\'s your birthday We\\'re gonna have a good time I\\'m glad it\\'s your birthday Happy birthday to you. Yes we\\'re going to a party party Yes we\\'re going to a party party Yes we\\'re going to a party party. I would like you to dance--Birthday Take a cha-cha-cha-chance-Birthday I would like you to dance--Birthday Dance You say it\\'s your birthday Well it\\'s my birthday too--yeah You say it\\'s your birthday We\\'re gonna have a good time I\\'m glad it\\'s your birthday Happy birthday to you. Because the world is round it turns me on Because the world is round...aaaaaahhhhhh Because the wind is high it blows my mind Because the wind is high...aaaaaaaahhhh Love is old, love is new Love is all, love is you Because the sky is blue, it makes me cry Because the sky is blue...aaaaaaahhhh Aaaaahhhhhhhhhh... Picture yourself in a boat on a river With tangerine trees and marmalade skies Somebody calls you, you answer quite slowly A girl with kaleidoscope eyes Cellophane flowers of yellow and green Towering over your head Look for the girl with the sun in her eyes And she\\'s gone Lucy in the sky with diamonds Lucy in the sky with diamonds Lucy in the sky with diamonds Ah Follow her down to a bridge by a fountain Where rocking horse people eat marshmallow pies Everyone smiles as you drift past the flowers That grow so incredibly high Newspaper taxis appear on the shore Waiting to take you away Climb in the back with your head in the clouds And you\\'re gone Lucy in the sky with diamonds Lucy in the sky with diamonds Lucy in the sky with diamonds Ah Picture yourself on a train in a station With plasticine porters with looking glass ties Suddenly someone is there at the turnstile The girl with the kaleidoscope eyes Lucy in the sky with diamonds Lucy in the sky with diamonds Lucy in the sky with diamonds Ah Lucy in the sky with diamonds Lucy in the sky with diamonds Lucy in the sky with diamonds Ah Lucy in the sky with diamonds Lucy in the sky with diamonds Lucy in the sky with diamonds A bad little kid moved into my neighborhood He won\\'t do nothing right just sitting down and look so good He don\\'t want to go to school and learn to read and write Just sits around the house and plays the rock and roll music all night Well, he put some tacks on teachers chair Puts chewing gum in little girl\\'s hair Man, junior, behave yourself Buy every rock and roll book on the magazine stand Every dime that he get is lost to the jukebox man Well, he worries his teacher till at night she\\'s ready to poop From rocking and a-rolling spinning in a hula hoop Well, this rock and roll has got to stop Junior\\'s head is hard as rock Now junior, behave yourself Going tell your mama you better do what she said Get to the barber shop and get that hair cut off your head He took your canary and he fed it to the neighbors cat He gave the cocker spaniel a bath in mother\\'s laundromat Well, mama\\'s head has got to stop Junior\\'s head is hard as rock Now junior, behave yourself I give her all my love That\\'s all I do And if you saw my love You\\'d love her to I love her She gives my everything And tenderly The kiss my lover brings She brings to me And I love her A love like ours Could never die As long as I Have you near me Bright are the stars that shine Dark is the sky I know this love of mine Will never die And I love her Bright are the stars that shine Dark is the sky I know this love of mine Will never die And I love her, ooh You say \"Yes\", I say \"No\". You say \"Stop\" and I say \"Go, go, go\". Oh no. You say \"Goodbye\" and I say \"Hello, hello, hello\". I don\\'t know why you say \"Goodbye\", I say \"Hello, hello, hello\". I don\\'t know why you say goodbye, I say hello. I say \"High\", you say \"Low\". You say \"Why?\" And I say \"I don\\'t know\". Oh no. You say \"Goodbye\" and I say \"Hello, hello, hello\". I don\\'t know why you say \"Goodbye\", I say \"Hello, hello, hello\". (Hello, goodbye, hello, goodbye. Hello, goodbye.) I don\\'t know why you say \"Goodbye\", I say \"Hello\". (Hello, goodbye, hello, goodbye. Hello, goodbye. Hello, goodbye.) Why, why, why, why, why, why, do you Say \"Goodbye, goodbye, bye, bye\". Oh no. You say \"Goodbye\" and I say \"Hello, hello, hello\". I don\\'t know why you say \"Goodbye\", I say \"Hello, hello, hello\". I don\\'t know why you say \"Goodbye\", I say \"Hello\". You say \"Yes\", I say \"No\". (I say \"Yes\", but I may mean \"No\"). You say \"Stop\", I say \"Go, go, go\". (I can stay still it\\'s time to go). Oh, oh no. You say \"Goodbye\" and I say \"Hello, hello, hello\". I don\\'t know why you say \"Goodbye\", I say \"Hello, hello, hello\". I don\\'t know why you say \"Goodbye\", I say \"Hello, hello, hello\". I don\\'t know why you say \"Goodbye\", I say \"Hello-wow, oh. Hello\". Hela, heba, helloa. Hela, heba, helloa. Hela, heba, helloa. Hela, heba, helloa. (Hela.) Hela, heba, helloa. Hela, heba, helloa. Hela, heba, helloa. Hela, heba, helloa. Hela, heba, helloa. I once had a girl Or should I say she once had me She showed me her room Isn\\'t it good Norwegian wood? She asked me to stay And she told me to sit anywhere So I looked around And I noticed there wasn\\'t a chair I sat on a rug biding my time Drinking her wine We talked until two and then she said \"It\\'s time for bed\" She told me she worked In the morning and started to laugh I told her I didn\\'t And crawled off to sleep in the bath And when I awoke I was alone This bird had flown So I lit a fire Isn\\'t it good Norwegian wood? Well, shake it up baby now Twist and shout Come on, come on, come, come on baby now Come on and work it on out Well, work it on out, honey You know you look so good You know you got me goin\\' now Just like I know you would Well, shake it up baby now Twist and shout Come on, come on, come, come on baby now Come on and work it on out You know you twist, little girl You know you twist so fine Come on and twist a little closer now And let me know that you\\'re mine, woo Ah, ah, ah, ah, wow Baby now Twist and shout Come on, come on, come, come on baby now Come on and work it on out You know you twist, little girl You know you twist so fine Come on and twist a little closer now And let me know that you\\'re mine Well, shake it, shake it, shake it, baby now Well, shake it, shake it, shake it, baby now Well, shake it, shake it, shake it, baby now Ah, ah, ah, ah She loves you, yeah, yeah, yeah She loves you, yeah, yeah, yeah She loves you, yeah, yeah, yeah You think you\\'ve lost your love Well, I saw her yesterday-yi-yay It\\'s you she\\'s thinking of And she told me what to say-yi-yay She says she loves you and you know that can\\'t be bad Yes, she loves you and you know you should be glad She said you hurt her so She almost lost her mind And now she says she knows You\\'re not the hurting kind She says she loves you and you know that can\\'t be bad Yes, she loves you and you know you should be glad Oo, she loves you, yeah, yeah, yeah She loves you, yeah, yeah, yeah With a love like that You know you should be glad You know it\\'s up to you I think it\\'s only fair Pride can hurt you too Apologize to her Because she loves you and you know that can\\'t be bad Yes, she loves you and you know you should be glad Oo, she loves you, yeah, yeah, yeah She loves you, yeah, yeah, yeah With a love like that You know you should be glad With a love like that you know you should be glad With a love like that you know you should be glad Yeah, yeah, yeah, Yeah, yeah, yeah, yeah Ah look at all the lonely people Ah look at all the lonely people Eleanor Rigby, picks up the rice In the church where a wedding has been Lives in a dream Waits at the window, wearing the face That she keeps in a jar by the door Who is it for All the lonely people Where do they all come from? All the lonely people Where do they all belong? Father McKenzie, writing the words Of a sermon that no one will hear No one comes near Look at him working, darning his socks In the night when there\\'s nobody there What does he care All the lonely people Where do they all come from? All the lonely people Where do they all belong? Ah look at all the lonely people Ah look at all the lonely people Eleanor Rigby, died in the church And was buried along with her name Nobody came Father McKenzie, wiping the dirt From his hands as he walks from the grave No one was saved All the lonely people Where do they all come from? All the lonely people Where do they all belong? Well she was just seventeen You know what I mean And the way she looked Was way beyond compare So how could I dance with another, Oh, when I saw her standing there Well she looked at me And I, I could see That before too long I\\'d fall in love with her She wouldn\\'t dance with another Oh, when I saw her standing there Well my heart went boom When I crossed that room And I held her hand in mine Oh we danced through the night And we held each other tight And before too long I fell in love with her Now I\\'ll never dance with another Oh, when I saw her standing there Well my heart went boom When I crossed that room And I held her hand in mine Oh we danced through the night And we held each other tight And before too long I fell in love with her Now I\\'ll never dance with another Oh, when I saw her standing there Oh, since I saw her standing there Yeah, well since I saw her standing there Close your eyes and I\\'ll kiss you Tomorrow I\\'ll miss you Remember I\\'ll always be true And then while I\\'m away I\\'ll write home every day And I\\'ll send all my loving to you I\\'ll pretend that I\\'m kissing The lips I am missing And hope that my dreams will come true And then while I\\'m away I\\'ll write home every day And I\\'ll send all my loving to you All my loving, I will send to you All my loving, darling I\\'ll be true Close your eyes and I\\'ll kiss you Tomorrow I\\'ll miss you Remember I\\'ll always be true And then while I\\'m away I\\'ll write home every day And I\\'ll send all my loving to you All my loving, I will send to you All my loving, darling I\\'ll be true All my loving, all my loving Woo, all my loving, I will send to you It\\'s been a hard day\\'s night, and I been working like a dog It\\'s been a hard day\\'s night, I should be sleeping like a log But when I get home to you I\\'ll find the things that you do Will make me feel alright You know I work all day to get you money to buy you things And it\\'s worth it just to hear you say you\\'re going to give me everything So why on earth should I moan, \\'cause when I get you alone You know I feel ok When I\\'m home everything seems to be right When I\\'m home feeling you holding me tight, tight Owww! So why on earth should I moan, \\'cause when I get you alone You know I feel ok When I\\'m home everything seems to be right When I\\'m home feeling you holding me tight, tight, yeah It\\'s been a hard day\\'s night, and I been working like a dog It\\'s been a hard day\\'s night, I should be sleeping like a log But when I get home to you I\\'ll find the things that you do Will make me feel alright You know I feel alright You know I feel alright... Love, love me do You know I love you I\\'ll always be true So please, love me do Whoa, love me do Love, love me do You know I love you I\\'ll always be true So please, love me do Whoa, love me do Someone to love Somebody new Someone to love Someone like you Love, love me do You know I love you I\\'ll always be true So please, love me do Whoa, love me do Love, love me do You know I love you I\\'ll always be true So please, love me do Whoa, love me do Yeah, love me do Whoa, oh, love me do Good day sunshine, Good day sunshine, Good day sunshine. I need to laugh, and when the sun is out I\\'ve got something I can laugh about, I feel good, in a special way. I\\'m in love and it\\'s a sunny day. Good day sunshine, Good day sunshine, Good day sunshine. We take a walk, the sun is shining down, Burns my feet as they touch the ground. Good day sunshine, Good day sunshine, Good day sunshine. And then we lie, beneath a shady tree, I love her and she\\'s loving me. She feels good, she knows she\\'s looking fine. I\\'m so proud to know that she is mine. Good day sunshine, Good day sunshine, Good day sunshine. Good day sunshine, Good day sunshine, Good day sunshine, Good day sunshine. Can\\'t buy me love, love Can\\'t buy me love I\\'ll buy you a diamond ring my friend if it makes you feel alright I\\'ll get you anything my friend if it makes you feel alright Cos I don\\'t care too much for money, and money can\\'t buy me love I\\'ll give you all I got to give if you say you\\'ll love me too I may not have a lot to give but what I got I\\'ll give to you I don\\'t care too much for money, money can\\'t buy me love Can\\'t buy me love, everybody tells me so Can\\'t buy me love, no no no, no Say you don\\'t need no diamond ring and I\\'ll be satisfied Tell me that you want the kind of thing that money just can\\'t buy I don\\'t care too much for money, money can\\'t buy me love Owww Can\\'t buy me love, everybody tells me so Can\\'t buy me love, no no no, no Say you don\\'t need no diamond ring and I\\'ll be satisfied Tell me that you want the kind of thing that money just can\\'t buy I don\\'t care too much for money, money can\\'t buy me love Can\\'t buy me love, love Can\\'t buy me love When I get older losing my hair, Many years from now. Will you still be sending me a Valentine. Birthday greetings bottle of wine. If I\\'d been out till quarter to three. Would you lock the door. Will you still need me, will you still feed me, When I\\'m sixty-four. You\\'ll be older too, And if I say the word, I could stay with you. I could be handy, mending a fuse When your lights have gone. You can knit a sweater by the fireside Sunday morning go for a ride, Doing the garden, digging the weeds, Who could ask for more. Will you still need me, will you still feed me When I\\'m sixty-four. Every summer we can rent a cottage, In the Isle of Wight, if it\\'s not too dear We shall scrimp and save Grandchildren on your knee Vera Chuck&Dave; Send me a postcard, drop me a line, Stating point of view Indicate precisely what you mean to say Your\\'s sincerely wasting away Give me your answer, fill in a form Mine for evermore Will you still need me, will you still feed me When I\\'m sixty-four It was twenty years ago today, Sgt. Pepper taught the band to play, They\\'ve been going in and out of style But they\\'re guaranteed to raise a smile. So may I introduce to you The act you\\'ve known for all these years Sgt. Pepper\\'s Lonely Hearts Club Band. We\\'re Sgt. Pepper\\'s Lonely Hearts Club Band We hope you will enjoy the show, We\\'re Sgt. Pepper\\'s Lonely Hearts Club Band Sit back and let the evening go. Sgt. Pepper\\'s lonely, Sgt. Pepper\\'s lonely, Sgt. Pepper\\'s Lonely Hearts Club Band It\\'s wonderful to be here It\\'s certainly a thrill You\\'re such a lovely audience We\\'d like to take you home with us We\\'d love to take you home I don\\'t really like to stop the show But I thought that you might like to know That the singer\\'s going to sing a song And he wants you all to sing along So let me introduce to you The one and only Billy Shears And Sgt. Pepper\\'s Lonely Hearts Club Band. To lead a better life I need my love to be here Here, making each day of the year Changing my life with a wave of her hand Nobody can deny that there\\'s something there There, running my hands through her hair Both of us thinking how good it can be Someone is speaking, but she doesn\\'t know he\\'s there I want her everywhere And if she\\'s beside me I know I need never care But to love her is to need her everywhere Knowing that love is to share Each one believing that love never dies Watching their eyes and hoping I\\'m always there I want her everywhere And if she\\'s beside me I know I need never care But to love her is to need her everywhere Knowing that love is to share Each one believing that love never dies Watching their eyes and hoping I\\'m always there I will be there And everywhere Here, there and everywhere I\\'ve just seen a face, I can\\'t forget the time or place Where we just met. She\\'s just the girl for me And I want all the world to see We\\'ve met, mm-mm-mm-m\\'mm-mm Had it been another day I might have looked the other way And I\\'d have never been aware. But as it is I\\'ll dream of her Tonight, di-di-di-di\\'n\\'di. Falling, yes I am falling, And she keeps calling Me back again. I have never known The like of this, I\\'ve been alone And I have missed things And kept out of sight But other girls were never quite Like this, di-di-di-di\\'n\\'di. Falling, yes I am falling, And she keeps calling Me back again. Yeah! Bup-a-lup-bup! Falling, yes I am falling, And she keeps calling Me back again. I\\'ve just seen a face, I can\\'t forget the time or place Where we just met. She\\'s just the girl for me And I want all the world to see We\\'ve met, mm-mm-mm-di-di-di. Falling, yes I am falling, And she keeps calling Me back again. Falling, yes I am falling, And she keeps calling Me back again. Oh, falling, yes I am falling, And she keeps calling Me back again. Imagine there\\'s no heaven It\\'s easy if you try No hell below us Above us only sky Imagine all the people Living for today... Imagine there\\'s no countries It isn\\'t hard to do Nothing to kill or die for And no religion, too Imagine all the people Living life in peace You, you may say I\\'m a dreamer But I\\'m not the only one I hope someday you will join us And the world will be as one Imagine no possessions I wonder if you can No need for greed or hunger A brotherhood of man Imagine all the people Sharing all the world You, you may say I\\'m a dreamer But I\\'m not the only one I hope someday you will join us And the world will live as one The long and winding road That leads to your door Will never disappear I\\'ve seen that road before It always leads me here Lead me to you door The wild and windy night That the rain washed away Has left a pool of tears Crying for the day Why leave me standing here Let me know the way Many times I\\'ve been alone And many times I\\'ve cried Any way you\\'ll never know The many ways I\\'ve tried But still they lead me back To the long winding road You left me waiting here A long long time ago Don\\'t leave me standing here Lead me to your door But still they lead me back To the long winding road You left me waiting here A long long time ago Don\\'t leave me standing here Lead me to your door Oh, I need your love, babe Guess you know it\\'s true Hope you need my love babe Just like I need you Hold me, love me, hold me, love me I ain\\'t got nothing but love, babe Eight days a week Love you every day, girl Always on my mind One thing I can say, girl Love you all the time Hold me, love me, hold me, love me I ain\\'t got nothing but love, girl Eight days a week Eight days a week I love you Eight days a week Is not enough to show I care Ooh I need your love, babe Guess you know it\\'s true Hope you need my love babe Just like I need you Hold me, love me, hold me, love me I ain\\'t got nothing but love, babe Eight days a week Eight days a week I love you Eight days a week is not enough to show I care Love you every day, girl Always on my mind One thing I can say girl Love you all the time Hold me, love me, hold me, love me I ain\\'t got nothing but love, babe Eight days a week Eight days a week Eight days a week Is there anybody going to listen to my story All about the girl who came to stay? She\\'s the kind of girl You want so much it make you sorry Still you don\\'t regret a single day Ah, girl, girl, girl When I think of all the times I tried so hard to leave her She will turn to me and start to cry And she promises the earth to me And I believe her After all this time I don\\'t know why Ah, girl, girl, girl She\\'s the kind of girl who puts you down When friends are there You feel a fool When you say she\\'s looking good She acts as if it\\'s understood She\\'s cool, ooh, oo, oo, oo Girl, girl, girl Was she told when she was young That pain would lead to pleasure Did she understand it when they said That a man must break his back To earn his day of leisure? Will she still believe it when he\\'s dead Ah, girl, girl, girl Girl I\\'d like to be under the sea In an octopus\\' garden in the shade He\\'d let us in, knows where we\\'ve been In his octopus\\' garden in the shade I\\'d ask my friends to come and see An octopus\\' garden with me I\\'d like to be under the sea In an octopus\\' garden in the shade. We would be warm below the storm In our little hideaway beneath the waves Resting our head on the sea bed In an octopus\\' garden near a cave We would sing and dance around Because we know we can\\'t be found I\\'d like to be under the sea In an octopus\\' garden in the shade We would shout and swim about The coral that lies beneath the waves (Lies beneath the ocean waves) Oh what joy for every girl and boy Knowing they\\'re happy and they\\'re safe (Happy and they\\'re safe) We would be so happy you and me No one there to tell us what to do I\\'d like to be under the sea In an octopus\\' garden with you. One two three four Can I have a little more Five six seven eight nine ten I love you A B C D Can I bring my friend to tea E F G H I J I love you Bom bom bom bompa bom Sail the ship Bompa bom Chop the tree bompa bom Skip the rope bompa bom Look at me All together now All together now All together now All together now All together now All together now All together now All together now Black white green red Can I take my friend to bed Pink brown yellow orange and blue I love you All together now All together now All together now All together now All together now All together now All together now All together now All together now All together now All together now All together now All together now All together now All together now All together now Bom bom bom bompa bom Sail the ship Bompa bom Chop the tree bompa bom Skip the rope bompa bom Look at me All together now All together now All together now All together now All together now All together now All together now All together now All together now All together now All together now All together now All together now All together now All together now All together now I dig a pony Well, you can celebrate anything you want Yes, you can celebrate anything you want Oh I do a road hog Well, you can penetrate any place you go Yes, you can penetrate any place you go I told you so All I want is you Everything has got to be just like you want it to Because I pick a moon dog Well, you can radiate everything you are Yes, you can radiate everything you are Oh now I roll a stoney Well, you can imitate everyone you know Yes, you can imitate everyone you know I told you so All I want is you Everything has got to be just like you want it to Because (woo) Oh now I feel the wind blow Well, you can indicate everything you see Yes, you can indicate everything you see Oh now I load a lorry Well, you can syndicate any boat you row Yeah, you can syndicate any boat you row I told you so All I want is you Everything has got to be just like you want it to Because Oh! Darling, please believe me I\\'ll never do you no harm Believe me when I tell you I\\'ll never do you no harm Oh! Darling, if you leave me I\\'ll never make it alone Believe me when I beg you Don\\'t ever leave me alone When you told me you didn\\'t need me anymore Well you know I nearly broke down and cried When you told me you didn\\'t need me anymore Well you know I nearly broke down and died Oh! Darling, if you leave me I\\'ll never make it alone Believe me when I tell you I\\'ll never do you no harm When you told me you didn\\'t need me anymore Well you know I nearly broke down and cried When you told me you didn\\'t need me anymore Well you know I nearly broke down and died Oh! Darling, please believe me I\\'ll never let you down Believe me when I tell you I\\'ll never do you no harm If I fell in love with you Would you promise to be true And help me understand \\'Cause I\\'ve been in love before And I found that love was more Than just holding hands If I give my heart to you I must be sure From the very start That you would love me more than her If I trust in you oh please Don\\'t run and hide If I love you too oh please Don\\'t hurt my pride like her \\'Cause I couldn\\'t stand the pain And I would be sad if our new love was in vain So I hope you see that I Would love to love you And that she will cry When she learns we are two If I fell in love with you I\\'m so tired, I haven\\'t slept a wink I\\'m so tired, my mind is on the blink I wonder should I get up and fix myself a drink No, no, no I\\'m so tired, I don\\'t know what to do I\\'m so tired, my mind is set on you I wonder should I call you but I know what you would do You\\'d say I\\'m putting you on But it\\'s no joke, it\\'s doing me harm You know I can\\'t sleep, I can\\'t stop my brain You know it\\'s three weeks, I\\'m going insane You know I\\'d give you everything I\\'ve got For a little peace of mind I\\'m so tired, I\\'m feeling so upset Although I\\'m so tired, I\\'ll have another cigarette And curse Sir Walter Raleigh He was such a stupid get You\\'d say I\\'m putting you on But it\\'s no joke, it\\'s doing me harm You know I can\\'t sleep, I can\\'t stop my brain You know it\\'s three weeks, I\\'m going insane You know I\\'d give you everything I\\'ve got For a little peace of mind I\\'d give you everything I\\'ve got For a little peace of mind I\\'d give you everything I\\'ve got For a little peace of mind Sheepdog, standing in the rain Bullfrog, doing it again Some kind of happiness is Measured out in miles What makes you think you\\'re Something special when you smile Childlike no one understands Jackknife in your sweaty hands Some kind of innocence is Measured out in years You don\\'t know what it\\'s like To listen to your fears You can talk to me You can talk to me You can talk to me If you\\'re lonely, you can talk to me Big man (yeah) walking in the park Wigwam frightened of the dark Some kind of solitude is Measured out in you You think you know me, but you haven\\'t got a clue You can talk to me You can talk to me You can talk to me If you\\'re lonely, you can talk to me Hey hey Roar Hey, bulldog (hey bulldog) Woof Hey, bulldog Hey, bulldog Hey, bulldog Hey man Whats up brother? Roof What do ya say I say, roof You know any more? Ah ah (you got it, that\\'s it, you had it) That\\'s it man, wo ho, that\\'s it, you got it Woah Look at me man, I only had ten children Ah ah ah ah ah ah ha ha ha ha Quiet, quiet (ok) Quiet Hey, bulldog, hey bulldog Flew in from Miami Beach BOAC Didn\\'t get to bed last night On the way the paper bag was on my knee Man, I had a dreadful flight I\\'m back in the USSR You don\\'t know how lucky you are, boy Back in the USSR, yeah Been away so long I early knew the place Gee, it\\'s good to be back home Leave it till tomorrow to unpack my case Honey disconnect the phone I\\'m back in the USSR You don\\'t know how lucky you are, boy Back in the US Back in the US Back in the USSR Well the Ukraine girls really knock me out They leave the west behind And Moscow girls make me sing and shout That Georgia\\'s always on my my my my my my my my my mind Oh, come on Hu hey hu, hey, ah, yeah Yeah, yeah, yeah I\\'m back in the USSR You don\\'t know how lucky you are, boys Back in the USSR Well the Ukraine girls really knock me out They leave the west behind And Moscow girls make me sing and shout That Georgia\\'s always on my my my my my my my my my mind Oh, show me round your snow peaked Mountain way down south Take me to your daddy\\'s farm Let me hear your balalaika\\'s ringing out Come and keep your comrade warm I\\'m back in the USSR Hey, you don\\'t know how lucky you are, boy Back in the USSR Oh, let me tell you honey Day after day Alone on a hill The man with the foolish grin Is keeping perfectly still But nobody wants to know him They can see that he\\'s just a fool And he never gives an answer But the fool on the hill Sees the sun going down And the eyes in his head See the world spinning round Well on the way Head in a cloud The man of a thousand voices Talking perfectly loud But nobody ever hears him Or the sounds he appears to make And he never seems to notice But the fool on the hill Sees the sun going down And the eyes in his head See the world spinning round And nobody seems to like him They can tell what he wants to do And he never shows his feelings But the fool on the hill Sees the sun going down And the eyes in his head See the world spinning round He never listens to them He knows that they\\'re the fools They don\\'t like him The fool on the hill Sees the sun going down And the eyes in his head See the world spinning round Oh You Never Give Me Your Money You only give me your funny paper And in the middle of negotiations you break down I never give you my number I only give my situation And in the middle of investigation I break down Out of college, money spent See no future, pay no rent All the money\\'s gone, nowhere to go Any jobber got the sack Monday morning, turning back Yellow lorry slow, nowhere to go But, oh, that magic feeling Nowhere to go Oh, that magic feeling Nowhere to go Nowhere to go Ah Ah Ah One sweet dream Pick up the bags and get in the limousine Soon we\\'ll be away from here Step on the gas and wipe that tear away One sweet dream came true today Came true today Came true today Yes it did (na, na) One, two, three, four, five, six, seven All good children go to heaven One, two, three, four, five, six, seven All good children go to heaven One, two, three, four, five, six, seven All good children go to heaven One, two, three, four, five, six, seven All good children go to heaven One, two, three, four, five, six, seven All good children go to heaven One, two, three, four, five, six, seven All good children go to heaven One, two, three, four, five, six, seven All good children go to heaven One, two, three, four, five, six, seven All good children go to heaven One, two, three, four, five, six, seven All good children go to heaven Ah Here come the Sun King Here come the Sun King Everybody\\'s laughing Everybody\\'s happy Here come the Sun King Quando paramucho mi amore defelice corazon Mundo pararazzi mi amore chicka ferdy parasol Cuesto obrigado tanta mucho cake and eat it carousel Mean Mr. Mustard sleeps in the park Shaves in the dark, tryin to save paper Sleeps in a hole in the road Saving up to buy him some clothes Keeps a ten bob note up his nose Such a mean old man Such a mean old man His sister Pam works in the shop She never stops; she\\'s a go-getter Takes him out to look at the queen Only place that he\\'s ever been Always shouts out something obscene Such a dirty old man Dirty old man Well, you should see Polythene Pam She\\'s so good-looking, but she looks like a man Well, you should see her in drag Dressed in her polythene bag Yes, you should see Polythene Pam Yeah, yeah, yeah Get a dose of her in jackboots and kilt She\\'s killer-diller when she\\'s dressed to the hilt She\\'s the kind of a girl Who makes the News Of The World Yes, you could say that she\\'s attractively built Yeah, yeah, yeah (John) She\\'s coming in the house (Paul) Oh, look out! She Came In Through The Bathroom Window Protected by a silver spoon But now she sucks her thumb and wonders By the banks of her own lagoon Didn\\'t anybody tell her? Didn\\'t anybody see? Sunday\\'s on the phone to Monday Tuesday\\'s on the phone to me She said she\\'d always been a dancer She worked at fifteen clubs a day And though she thought I knew the answer Well, I knew what I could not say And so I quit the police department And got myself a steady job And though she tried her best to help me She could steal, but she could not rob Didn\\'t anybody tell her? Didn\\'t anybody see? Sunday\\'s on the phone to Monday Tuesday\\'s on the phone to me Oh, yeah Once there was a way To get back homeward Once there was a way to get back home Sleep, pretty darling; do not cry And I will sing a lullaby Golden Slumbers fill your eyes Smiles awake you when you rise Sleep pretty darling, do not cry And I will sing a lullaby. Once there was a way To get back homeward Once there was a way to get back home Sleep, pretty darling; do not cry And I will sing a lullaby Boy, you\\'re gonna Carry That Weight Carry That Weight a long time Boy, you\\'re gonna Carry That Weight Carry That Weight a long time I never give you my pillow I only send you my invitations And in the middle of the celebrations I break down Boy, you\\'re gonna Carry That Weight Carry That Weight a long time Boy, you\\'re gonna Carry That Weight Carry That Weight a long time Oh yeah! All right! Are you going to be in my dreams tonight? Love you, love you, love you, love you, love you, love you Love you, love you, love you, love you, love you, love you Love you, love you, love you, love you, love you, love you Love you, love you, love you, love you, love you, love you Love you, love you, love you, love you, love you, love you And, in The End The love you take Is equal to the love you make Her Majesty\\'s a pretty nice girl But she doesn\\'t have a lot to say Her majesty\\'s a pretty nice girl But she changes from day to day I want to tell her that I love her a lot But I got to get a bellyful of wine Her majesty\\'s a real nice girl Someday I\\'m going to make her mine Oh yeah, Someday I\\'m going to make her mine Two of us riding nowhere Spending someone\\'s Hard earned pay Two of us Sunday driving Not arriving On our way back home We\\'re on our way home We\\'re on our way home We\\'re going home Two of us sending postcards Writing letters On my wall You and me burning matches Lifting latches On our way back home We\\'re on our way home We\\'re on our way home We\\'re going home You and I have memories Longer than the road that stretches out ahead Two of us wearing raincoats Standing so low In the sun You and me chasing paper Getting nowhere On our way back home We\\'re on our way home We\\'re on our way home We\\'re going home Dear Prudence, won\\'t you come out to play Dear Prudence, greet the brand new day The sun is up, the sky is blue It\\'s beautiful and so are you Dear Prudence won\\'t you come out to play Dear Prudence open up your eyes Dear Prudence see the sunny skies The wind is low the birds will sing That you are part of everything Dear Prudence won\\'t you open up your eyes? Look around round Look around round round Look around Dear Prudence let me see you smile Dear Prudence like a little child The clouds will be a daisy chain So let me see you smile again Dear Prudence won\\'t you let me see you smile? Dear Prudence, won\\'t you come out to play Dear Prudence, greet the brand new day The sun is up, the sky is blue It\\'s beautiful and so are you Dear Prudence won\\'t you come out to play I should have known better with a girl like you, That I would love everything that you do; and I do, Hey, hey, hey, and I do. Whoa, oh, I never realized what a kiss could be, This could only happen to me Can\\'t you see, can\\'t you see That when I tell you that I love you, oh, You\\'re gonna say you love me too, Hoo, hoo, hoo, hoo, oh And when I ask you to be mine, You\\'re gonna say you love me too So oh I should realized a lot of things before If this is love you\\'ve got to give me more Give me more, hey hey hey, give me more Whoa, oh, I never realized what a kiss could be, This could only happen to me Can\\'t you see, can\\'t you see That when I tell you that I love you, oh, You\\'re gonna say you love me too, Hoo, hoo, hoo, hoo, oh And when I ask you to be mine, You\\'re gonna say you love me too You love me too, you love me too, you love me too He\\'s a real nowhere man Sitting in his nowhere land Making all his nowhere plans for nobody Doesn\\'t have a point of view Knows not where he\\'s going to Isn\\'t he a bit like you and me? Nowhere man please listen You don\\'t know what you\\'re missing Nowhere man, The world is at your command He\\'s as blind as he can be Just sees what he wants to see Nowhere man, can you see me at all Nowhere man don\\'t worry Take your time, don\\'t hurry Leave it all till somebody else Lends you a hand Ah, la, la, la, la Doesn\\'t have a point of view Knows not where he\\'s going to Isn\\'t he a bit like you and me? Nowhere man please listen You don\\'t know what you\\'re missing Nowhere man, The world is at your command Ah, la, la, la, la He\\'s a real nowhere man Sitting in his nowhere land Making all his nowhere plans for nobody Making all his nowhere plans for nobody Making all his nowhere plans for nobody Now somewhere in the Black Mountain Hills of Dakota There lived a young boy named Rocky Raccoon And one day his woman ran off with another guy Hit young Rocky in the eye Rocky didn\\'t like that He said, \"I\\'m gonna get that boy\" So one day he walked into town Booked himself a room in the local saloon Rocky Raccoon checked into his room Only to find Gideon\\'s Bible Rocky had come, equipped with a gun To shoot off the legs of his rival His rival it seems, had broken his dreams By stealing the girl of his fancy Her name was Magill, and she called herself Lil But everyone knew her as Nancy Now she and her man, who called himself Dan Were in the next room at the hoe down Rocky burst in, and grinning a grin He said, \"Danny boy, this is a showdown\" But Daniel was hot, he drew first and shot And Rocky collapsed in the corner Now the doctor came in, stinking of gin And proceeded to lie on the table He said, \"Rocky, you met your match\" And Rocky said, \"Doc, it\\'s only a scratch And I\\'ll be better, I\\'ll be better, Doc, as soon as I am able\" Now Rocky Raccoon, he fell back in his room Only to find Gideon\\'s Bible Gideon checked out, and he left it, no doubt To help with good Rocky\\'s revival Try to see it my way, Do I have to keep on talking till I can\\'t go on? While you see it your way, Run the risk of knowing that our love may soon be gone We can work it out, We can work it out Think of what you\\'re saying You can get it wrong and still you think that it\\'s alright Think of what I\\'m saying, We can work it out and get it straight, or say good night We can work it out, We can work it out Life is very short, and there\\'s no time For fussing and fighting, my friend I have always thought that it\\'s a crime, So I will ask you once again Try to see it my way, Only time will tell if I am right or I am wrong While you see it your way There\\'s a chance that we may fall apart before too long We can work it out, We can work it out Life is very short, and there\\'s no time For fussing and fighting, my friend I have always thought that it\\'s a crime, So I will ask you once again Try to see it my way, Only time will tell if I am right or I am wrong While you see it your way There\\'s a chance that we may fall apart before too long We can work it out, We can work it out Joan was quizzical; studied pataphysical Science in the home. Late nights all alone with a test tube. Oh, oh, oh, oh. Maxwell Edison, majoring in medicine, Calls her on the phone. \"Can I take you out to the pictures, Joa, oa, oa, oan?\" But as she\\'s getting ready to go, A knock comes on the door. Bang! Bang! Maxwell\\'s silver hammer Came down upon her head. Bang! Bang! Maxwell\\'s silver hammer Made sure that she was dead. Back in school again Maxwell plays the fool again. Teacher gets annoyed. Wishing to avoid and unpleasant Sce, e, e, ene, She tells Max to stay when the class has gone away, So he waits behind Writing fifty times \"I must not be So, o, o, o\" But when she turns her back on the boy, He creeps up from behind. Bang! Bang! Maxwell\\'s silver hammer Came down upon her head. Bang! Bang! Maxwell\\'s silver hammer Made sure that she was dead. P. C. Thirty-one said, \"We caught a dirty one.\" Maxwell stands alone Painting testimonial pictures. Oh, oh, oh, oh. Rose and Valerie, screaming from the gallery Say he must go free (Maxwell must go free) The judge does not agree and he tells them So, o, o, o. But as the words are leaving his lips, A noise comes from behind. Bang! Bang! Maxwell\\'s silver hammer Came down upon his head. Bang! Bang! Maxwell\\'s silver hammer Made sure that he was dead. Whoa, oh, oh, oh. Silver hammer man Here I stand head in hand Turn my face to the wall If she\\'s gone I can\\'t go on Feeling two foot small Everywhere people stare Each and every day I can see them laugh at me And I hear them say Hey you\\'ve got to hide your love away Hey you\\'ve got to hide your love away How can I even try? I can never win Hearing them, seeing them In the state I\\'m in How could she say to me \"Love will find a way?\" Gather round all you clowns Let me hear you say Hey you\\'ve got to hide your love away Hey you\\'ve got to hide your love away Last night I said these words to my girl I know you never even try, girl Come on, (come on) come on, (come on) come on, (come on) come on, (come on) Please, please me, woah yeah, like I please you You don\\'t need me to show the way, love Why do I always have to say, love Come on, (come on) come on, (come on) come on, (come on) come on, (come on) Please, please me, woah yeah, like I please you I don\\'t want to sound complaining But you know there\\'s always rain in my heart (in my heart) I do all the pleasing with you It\\'s so hard to reason with you Woah yeah, why do you make me blue? Last night I said these words to my girl I know you never even try, girl Come on, (come on) come on, (come on) come on, (come on) come on, (come on) Please, please me, woah yeah, like I please you Me, woah yeah, like I please you Me, woah yeah, like I please you Let me tell you how it will be There\\'s one for you, nineteen for me \\'Cause I\\'m the taxman, yeah, I\\'m the taxman Should five per cent appear too small Be thankful I don\\'t take it all \\'Cause I\\'m the taxman, yeah I\\'m the taxman If you drive a car, I\\'ll tax the street, If you try to sit, I\\'ll tax your seat. If you get too cold I\\'ll tax the heat, If you take a walk, I\\'ll tax your feet. Don\\'t ask me what I want it for If you don\\'t want to pay some more \\'Cause I\\'m the taxman, yeah, I\\'m the taxman Now my advice for those who die Declare the pennies on your eyes \\'Cause I\\'m the taxman, yeah, I\\'m the taxman And you\\'re working for no one but me.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KlsYd7_uOez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e0500e-663d-413d-aae4-9d62b6e4e3a1"
      },
      "source": [
        "# Transformar el corpus a tokens\n",
        "tokens=text_to_word_sequence(corpus)\n",
        "# Vistazo general de los primeros tokens\n",
        "tokens[:20]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yesterday',\n",
              " 'all',\n",
              " 'my',\n",
              " 'troubles',\n",
              " 'seemed',\n",
              " 'so',\n",
              " 'far',\n",
              " 'away',\n",
              " 'now',\n",
              " 'it',\n",
              " 'looks',\n",
              " 'as',\n",
              " 'though',\n",
              " \"they're\",\n",
              " 'here',\n",
              " 'to',\n",
              " 'stay',\n",
              " 'oh',\n",
              " 'i',\n",
              " 'believe']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlqpZSJOJ1xQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a09a0a8d-238c-44c6-f374-dafbe542ac3c"
      },
      "source": [
        "print(\"Cantidad de tokens en el corpus:\", len(tokens))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de tokens en el corpus: 14387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhQevOynuYk2"
      },
      "source": [
        "# Código para hacer el desfazaje de las palabras\n",
        "# según el train_len\n",
        "text_sequences = []\n",
        "for i in range(train_len, len(tokens)):\n",
        "  seq = tokens[i-train_len:i]\n",
        "  text_sequences.append(seq)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU3FuqHSuhzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d8cb793-a9b9-45f5-e374-05e4e68b91ad"
      },
      "source": [
        "# Demos un vistazo a nuestros vectores para entrenar el modelo\n",
        "text_sequences[:20]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['yesterday', 'all', 'my', 'troubles'],\n",
              " ['all', 'my', 'troubles', 'seemed'],\n",
              " ['my', 'troubles', 'seemed', 'so'],\n",
              " ['troubles', 'seemed', 'so', 'far'],\n",
              " ['seemed', 'so', 'far', 'away'],\n",
              " ['so', 'far', 'away', 'now'],\n",
              " ['far', 'away', 'now', 'it'],\n",
              " ['away', 'now', 'it', 'looks'],\n",
              " ['now', 'it', 'looks', 'as'],\n",
              " ['it', 'looks', 'as', 'though'],\n",
              " ['looks', 'as', 'though', \"they're\"],\n",
              " ['as', 'though', \"they're\", 'here'],\n",
              " ['though', \"they're\", 'here', 'to'],\n",
              " [\"they're\", 'here', 'to', 'stay'],\n",
              " ['here', 'to', 'stay', 'oh'],\n",
              " ['to', 'stay', 'oh', 'i'],\n",
              " ['stay', 'oh', 'i', 'believe'],\n",
              " ['oh', 'i', 'believe', 'in'],\n",
              " ['i', 'believe', 'in', 'yesterday'],\n",
              " ['believe', 'in', 'yesterday', 'suddenly']]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "064N2jtLvHRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0c1eeea-b007-4d5c-b054-ed7a4ce34832"
      },
      "source": [
        "# Proceso de tokenizacion\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(text_sequences)\n",
        "\n",
        "# Convertimos las palabras a números\n",
        "# entran palabras -> salen números\n",
        "sequences = tok.texts_to_sequences(text_sequences)\n",
        "\n",
        "# Damos un vistazo\n",
        "sequences[:20]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[216, 10, 12, 907],\n",
              " [10, 12, 907, 908],\n",
              " [12, 907, 908, 42],\n",
              " [907, 908, 42, 909],\n",
              " [908, 42, 909, 121],\n",
              " [42, 909, 121, 22],\n",
              " [909, 121, 22, 13],\n",
              " [121, 22, 13, 638],\n",
              " [22, 13, 638, 91],\n",
              " [13, 638, 91, 217],\n",
              " [638, 91, 217, 349],\n",
              " [91, 217, 349, 79],\n",
              " [217, 349, 79, 4],\n",
              " [349, 79, 4, 314],\n",
              " [79, 4, 314, 23],\n",
              " [4, 314, 23, 2],\n",
              " [314, 23, 2, 160],\n",
              " [23, 2, 160, 9],\n",
              " [2, 160, 9, 216],\n",
              " [160, 9, 216, 520]]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwsvmvDKKXSP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523e5091-5956-4ee3-a71f-73d0c640d1d3"
      },
      "source": [
        "print(\"Cantidad de rows del dataset:\", len(sequences))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de rows del dataset: 14383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMVP4bj0vL2e"
      },
      "source": [
        "### 3 - Input y target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx2xwdz3KloJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d35c51c-8a8b-4357-b1b9-d98b92801a32"
      },
      "source": [
        "# Con numpy es muy fácil realizar el slicing de vectores\n",
        "ex = np.array([[1,2,3,4],[5,6,7,8]])\n",
        "ex"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3, 4],\n",
              "       [5, 6, 7, 8]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEod7qghvTVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34cb3c6e-6c9e-46e7-893a-e7116b94478b"
      },
      "source": [
        "# Con numpy es muy fácil realizar el slicing de vectores\n",
        "print(\"Dimension:\", ex.shape)\n",
        "print(\"Todos los elementos:\", ex)\n",
        "print(\"Todos los elementos menos el último:\", ex[:, :-1])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension: (2, 4)\n",
            "Todos los elementos: [[1 2 3 4]\n",
            " [5 6 7 8]]\n",
            "Todos los elementos menos el último: [[1 2 3]\n",
            " [5 6 7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i95xWqtCvp8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50170ce0-1dbc-44a0-a06e-6d39090e5af4"
      },
      "source": [
        "input = ex[:,:-1] # todos los rows, menos la ultima col\n",
        "target = ex[:, -1] # última col de cada row\n",
        "\n",
        "print(\"Input:\", input)\n",
        "print(\"Target:\", target)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [[1 2 3]\n",
            " [5 6 7]]\n",
            "Target: [4 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1vJTG65v4Qn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f89eab-80ce-44f0-88ee-087a3817bb53"
      },
      "source": [
        "arr_sequences = np.array(sequences)\n",
        "x_data = arr_sequences[:,:-1]\n",
        "y_data_int = arr_sequences[:,-1] # aún falta el oneHotEncoder\n",
        "\n",
        "print(x_data.shape)\n",
        "print(y_data_int.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14383, 3)\n",
            "(14383,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln6kVWVlwBBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26deaec-5eef-4c31-cc58-748bb0011390"
      },
      "source": [
        "# Palabras del vocabulario\n",
        "tok.index_word"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'you',\n",
              " 2: 'i',\n",
              " 3: 'the',\n",
              " 4: 'to',\n",
              " 5: 'and',\n",
              " 6: 'love',\n",
              " 7: 'a',\n",
              " 8: 'me',\n",
              " 9: 'in',\n",
              " 10: 'all',\n",
              " 11: 'nah',\n",
              " 12: 'my',\n",
              " 13: 'it',\n",
              " 14: 'know',\n",
              " 15: 'be',\n",
              " 16: 'that',\n",
              " 17: 'of',\n",
              " 18: 'on',\n",
              " 19: 'she',\n",
              " 20: 'is',\n",
              " 21: 'say',\n",
              " 22: 'now',\n",
              " 23: 'oh',\n",
              " 24: 'yeah',\n",
              " 25: 'your',\n",
              " 26: 'get',\n",
              " 27: \"don't\",\n",
              " 28: 'back',\n",
              " 29: 'let',\n",
              " 30: 'with',\n",
              " 31: 'her',\n",
              " 32: 'can',\n",
              " 33: \"it's\",\n",
              " 34: 'do',\n",
              " 35: \"i'm\",\n",
              " 36: 'when',\n",
              " 37: 'come',\n",
              " 38: 'will',\n",
              " 39: 'he',\n",
              " 40: 'but',\n",
              " 41: 'for',\n",
              " 42: 'so',\n",
              " 43: 'need',\n",
              " 44: 'there',\n",
              " 45: 'like',\n",
              " 46: 'together',\n",
              " 47: 'no',\n",
              " 48: \"i'll\",\n",
              " 49: 'see',\n",
              " 50: 'was',\n",
              " 51: 'never',\n",
              " 52: 'we',\n",
              " 53: 'down',\n",
              " 54: 'way',\n",
              " 55: 'if',\n",
              " 56: 'want',\n",
              " 57: 'out',\n",
              " 58: 'hey',\n",
              " 59: 'one',\n",
              " 60: 'got',\n",
              " 61: 'go',\n",
              " 62: 'well',\n",
              " 63: \"can't\",\n",
              " 64: 'girl',\n",
              " 65: 'good',\n",
              " 66: 'his',\n",
              " 67: 'sun',\n",
              " 68: 'they',\n",
              " 69: 'home',\n",
              " 70: 'hello',\n",
              " 71: 'day',\n",
              " 72: 'are',\n",
              " 73: 'ah',\n",
              " 74: 'right',\n",
              " 75: 'little',\n",
              " 76: 'at',\n",
              " 77: 'from',\n",
              " 78: 'man',\n",
              " 79: 'here',\n",
              " 80: 'help',\n",
              " 81: 'gonna',\n",
              " 82: 'have',\n",
              " 83: 'not',\n",
              " 84: 'world',\n",
              " 85: 'going',\n",
              " 86: 'what',\n",
              " 87: 'still',\n",
              " 88: 'how',\n",
              " 89: 'too',\n",
              " 90: 'please',\n",
              " 91: 'as',\n",
              " 92: \"you're\",\n",
              " 93: 'just',\n",
              " 94: 'time',\n",
              " 95: 'make',\n",
              " 96: 'long',\n",
              " 97: 'yes',\n",
              " 98: 'yellow',\n",
              " 99: 'life',\n",
              " 100: 'why',\n",
              " 101: 'think',\n",
              " 102: 'only',\n",
              " 103: 'been',\n",
              " 104: 'once',\n",
              " 105: 'tell',\n",
              " 106: 'sky',\n",
              " 107: 'people',\n",
              " 108: 'eyes',\n",
              " 109: 'where',\n",
              " 110: \"she's\",\n",
              " 111: 'up',\n",
              " 112: 'jude',\n",
              " 113: 'take',\n",
              " 114: 'submarine',\n",
              " 115: 'sing',\n",
              " 116: 'give',\n",
              " 117: 'nowhere',\n",
              " 118: 'hold',\n",
              " 119: 'look',\n",
              " 120: 'goodbye',\n",
              " 121: 'away',\n",
              " 122: 'friends',\n",
              " 123: 'darling',\n",
              " 124: 'lonely',\n",
              " 125: 'boy',\n",
              " 126: 'nothing',\n",
              " 127: 'always',\n",
              " 128: 'hand',\n",
              " 129: \"we're\",\n",
              " 130: 'am',\n",
              " 131: 'goo',\n",
              " 132: 'should',\n",
              " 133: 'comes',\n",
              " 134: 'night',\n",
              " 135: 'feel',\n",
              " 136: 'some',\n",
              " 137: 'could',\n",
              " 138: 'had',\n",
              " 139: 'better',\n",
              " 140: 'them',\n",
              " 141: 'would',\n",
              " 142: 'head',\n",
              " 143: \"i've\",\n",
              " 144: 'more',\n",
              " 145: 'four',\n",
              " 146: 'buy',\n",
              " 147: 'mind',\n",
              " 148: 'carry',\n",
              " 149: 'this',\n",
              " 150: 'la',\n",
              " 151: 'everything',\n",
              " 152: 'two',\n",
              " 153: 'change',\n",
              " 154: 'by',\n",
              " 155: 'weight',\n",
              " 156: 'us',\n",
              " 157: 'our',\n",
              " 158: 'lucy',\n",
              " 159: 'sunshine',\n",
              " 160: 'believe',\n",
              " 161: 'an',\n",
              " 162: 'words',\n",
              " 163: 'before',\n",
              " 164: \"you'll\",\n",
              " 165: 'these',\n",
              " 166: 'pretty',\n",
              " 167: 'birthday',\n",
              " 168: \"i'd\",\n",
              " 169: 'three',\n",
              " 170: 'every',\n",
              " 171: 'diamonds',\n",
              " 172: 'true',\n",
              " 173: \"there's\",\n",
              " 174: 'said',\n",
              " 175: 'mm',\n",
              " 176: 'were',\n",
              " 177: 'forever',\n",
              " 178: 'dear',\n",
              " 179: 'mine',\n",
              " 180: 'nobody',\n",
              " 181: 'round',\n",
              " 182: 'leave',\n",
              " 183: 'money',\n",
              " 184: 'told',\n",
              " 185: 'work',\n",
              " 186: 'bom',\n",
              " 187: 'something',\n",
              " 188: 'hide',\n",
              " 189: 'standing',\n",
              " 190: 'then',\n",
              " 191: 'who',\n",
              " 192: 'waiting',\n",
              " 193: 'about',\n",
              " 194: \"'cause\",\n",
              " 195: 'children',\n",
              " 196: 'saw',\n",
              " 197: \"didn't\",\n",
              " 198: 'try',\n",
              " 199: 'loves',\n",
              " 200: \"nothing's\",\n",
              " 201: 'gone',\n",
              " 202: 'things',\n",
              " 203: 'stop',\n",
              " 204: 'sleep',\n",
              " 205: 'cry',\n",
              " 206: 'hard',\n",
              " 207: 'di',\n",
              " 208: \"he's\",\n",
              " 209: 'days',\n",
              " 210: 'alone',\n",
              " 211: 'because',\n",
              " 212: 'falling',\n",
              " 213: 'again',\n",
              " 214: 'prudence',\n",
              " 215: 'bang',\n",
              " 216: 'yesterday',\n",
              " 217: 'though',\n",
              " 218: 'came',\n",
              " 219: 'around',\n",
              " 220: 'mean',\n",
              " 221: 'has',\n",
              " 222: 'till',\n",
              " 223: 'juba',\n",
              " 224: 'shake',\n",
              " 225: 'loving',\n",
              " 226: 'eight',\n",
              " 227: 'five',\n",
              " 228: 'play',\n",
              " 229: 'place',\n",
              " 230: 'find',\n",
              " 231: 'may',\n",
              " 232: 'someone',\n",
              " 233: 'dead',\n",
              " 234: 'years',\n",
              " 235: 'send',\n",
              " 236: 'strawberry',\n",
              " 237: 'fields',\n",
              " 238: 'or',\n",
              " 239: 'band',\n",
              " 240: 'door',\n",
              " 241: 'happy',\n",
              " 242: 'garden',\n",
              " 243: 'anybody',\n",
              " 244: 'today',\n",
              " 245: \"won't\",\n",
              " 246: 'live',\n",
              " 247: 'blue',\n",
              " 248: 'while',\n",
              " 249: 'hela',\n",
              " 250: 'heaven',\n",
              " 251: 'week',\n",
              " 252: 'six',\n",
              " 253: 'seven',\n",
              " 254: 'rocky',\n",
              " 255: 'such',\n",
              " 256: 'easy',\n",
              " 257: 'answer',\n",
              " 258: 'into',\n",
              " 259: 'dance',\n",
              " 260: 'heart',\n",
              " 261: 'lullaby',\n",
              " 262: 'high',\n",
              " 263: 'face',\n",
              " 264: 'him',\n",
              " 265: 'does',\n",
              " 266: 'ask',\n",
              " 267: 'somebody',\n",
              " 268: 'any',\n",
              " 269: 'feeling',\n",
              " 270: 'knows',\n",
              " 271: 'talk',\n",
              " 272: \"you've\",\n",
              " 273: 'another',\n",
              " 274: 'belonged',\n",
              " 275: 'glad',\n",
              " 276: 'heba',\n",
              " 277: 'helloa',\n",
              " 278: 'baby',\n",
              " 279: 'twist',\n",
              " 280: 'kind',\n",
              " 281: 'care',\n",
              " 282: 'jai',\n",
              " 283: 'guru',\n",
              " 284: 'deva',\n",
              " 285: 'bad',\n",
              " 286: 'fool',\n",
              " 287: 'found',\n",
              " 288: 'loved',\n",
              " 289: 'fly',\n",
              " 290: 'seems',\n",
              " 291: 'fill',\n",
              " 292: 'done',\n",
              " 293: \"isn't\",\n",
              " 294: 'real',\n",
              " 295: 'much',\n",
              " 296: 'molly',\n",
              " 297: 'ever',\n",
              " 298: 'many',\n",
              " 299: 'understand',\n",
              " 300: 'sea',\n",
              " 301: 'beneath',\n",
              " 302: \"g'\",\n",
              " 303: 'joob',\n",
              " 304: 'penny',\n",
              " 305: 'lane',\n",
              " 306: 'keeps',\n",
              " 307: 'whoa',\n",
              " 308: 'sgt',\n",
              " 309: 'bompa',\n",
              " 310: 'ussr',\n",
              " 311: 'silver',\n",
              " 312: 'hoo',\n",
              " 313: 'taxman',\n",
              " 314: 'stay',\n",
              " 315: 'wisdom',\n",
              " 316: 'rain',\n",
              " 317: 'their',\n",
              " 318: 'remember',\n",
              " 319: 'made',\n",
              " 320: 'new',\n",
              " 321: 'went',\n",
              " 322: 'blackbird',\n",
              " 323: 'must',\n",
              " 324: 'desmond',\n",
              " 325: 'sweet',\n",
              " 326: 'last',\n",
              " 327: 'really',\n",
              " 328: 'smile',\n",
              " 329: 'show',\n",
              " 330: 'hear',\n",
              " 331: 'hmm',\n",
              " 332: 'crying',\n",
              " 333: 'sitting',\n",
              " 334: 'tired',\n",
              " 335: \"that's\",\n",
              " 336: 'behind',\n",
              " 337: 'room',\n",
              " 338: 'hope',\n",
              " 339: 'alright',\n",
              " 340: \"pepper's\",\n",
              " 341: 'lead',\n",
              " 342: 'everywhere',\n",
              " 343: 'road',\n",
              " 344: 'babe',\n",
              " 345: \"octopus'\",\n",
              " 346: 'bulldog',\n",
              " 347: 'woah',\n",
              " 348: 'hammer',\n",
              " 349: \"they're\",\n",
              " 350: 'wrong',\n",
              " 351: 'living',\n",
              " 352: 'through',\n",
              " 353: 'ears',\n",
              " 354: 'song',\n",
              " 355: 'under',\n",
              " 356: 'making',\n",
              " 357: 'changed',\n",
              " 358: 'lose',\n",
              " 359: 'singing',\n",
              " 360: 'learn',\n",
              " 361: 'dark',\n",
              " 362: 'homeward',\n",
              " 363: 'dream',\n",
              " 364: 'hair',\n",
              " 365: 'feed',\n",
              " 366: 'sixty',\n",
              " 367: 'doing',\n",
              " 368: \"somethin'\",\n",
              " 369: 'feet',\n",
              " 370: 'other',\n",
              " 371: 'old',\n",
              " 372: 'looking',\n",
              " 373: 'hill',\n",
              " 374: 'heard',\n",
              " 375: 'guitar',\n",
              " 376: 'gently',\n",
              " 377: 'weeps',\n",
              " 378: 'laugh',\n",
              " 379: 'fell',\n",
              " 380: 'michelle',\n",
              " 381: 'tres',\n",
              " 382: 'bien',\n",
              " 383: 'ensemble',\n",
              " 384: 'thought',\n",
              " 385: 'girls',\n",
              " 386: 'party',\n",
              " 387: 'shout',\n",
              " 388: 'each',\n",
              " 389: 'tight',\n",
              " 390: 'friend',\n",
              " 391: 'calling',\n",
              " 392: 'imagine',\n",
              " 393: 'harm',\n",
              " 394: 'phone',\n",
              " 395: \"maxwell's\",\n",
              " 396: 'o',\n",
              " 397: 'over',\n",
              " 398: 'times',\n",
              " 399: 'broken',\n",
              " 400: 'light',\n",
              " 401: 'until',\n",
              " 402: 'paper',\n",
              " 403: 'across',\n",
              " 404: 'waves',\n",
              " 405: 'wind',\n",
              " 406: 'sad',\n",
              " 407: 'moment',\n",
              " 408: 'free',\n",
              " 409: 'since',\n",
              " 410: 'smiles',\n",
              " 411: 'middle',\n",
              " 412: 'break',\n",
              " 413: 'dreams',\n",
              " 414: \"doesn't\",\n",
              " 415: 'says',\n",
              " 416: 'takes',\n",
              " 417: 'ob',\n",
              " 418: 'knee',\n",
              " 419: 'than',\n",
              " 420: 'thing',\n",
              " 421: 'walrus',\n",
              " 422: \"ain't\",\n",
              " 423: 'near',\n",
              " 424: 'turn',\n",
              " 425: 'seen',\n",
              " 426: 'lucky',\n",
              " 427: 'sure',\n",
              " 428: 'bed',\n",
              " 429: 'very',\n",
              " 430: 'skies',\n",
              " 431: 'sit',\n",
              " 432: 'knew',\n",
              " 433: 'left',\n",
              " 434: 'makes',\n",
              " 435: 'yourself',\n",
              " 436: 'rock',\n",
              " 437: 'spinning',\n",
              " 438: 'kiss',\n",
              " 439: 'die',\n",
              " 440: 'oo',\n",
              " 441: 'hearts',\n",
              " 442: 'club',\n",
              " 443: 'met',\n",
              " 444: 'peace',\n",
              " 445: 'sees',\n",
              " 446: \"wouldn't\",\n",
              " 447: 'speaking',\n",
              " 448: 'whisper',\n",
              " 449: 'chance',\n",
              " 450: 'tomorrow',\n",
              " 451: 'music',\n",
              " 452: 'universe',\n",
              " 453: 'opened',\n",
              " 454: 'start',\n",
              " 455: 'upon',\n",
              " 456: 'places',\n",
              " 457: 'lovers',\n",
              " 458: 'affection',\n",
              " 459: 'often',\n",
              " 460: 'arise',\n",
              " 461: 'black',\n",
              " 462: 'doo',\n",
              " 463: 'golden',\n",
              " 464: 'tonight',\n",
              " 465: 'end',\n",
              " 466: 'save',\n",
              " 467: 'known',\n",
              " 468: 'hung',\n",
              " 469: 'tree',\n",
              " 470: 'low',\n",
              " 471: 'chorus',\n",
              " 472: 'goes',\n",
              " 473: 'ring',\n",
              " 474: 'couple',\n",
              " 475: 'after',\n",
              " 476: 'older',\n",
              " 477: 'wine',\n",
              " 478: 'point',\n",
              " 479: 'view',\n",
              " 480: 'indicate',\n",
              " 481: 'first',\n",
              " 482: 'younger',\n",
              " 483: 'ground',\n",
              " 484: 'green',\n",
              " 485: 'ahead',\n",
              " 486: 'apart',\n",
              " 487: 'stand',\n",
              " 488: 'run',\n",
              " 489: 'eggman',\n",
              " 490: 'eggmen',\n",
              " 491: 'ho',\n",
              " 492: 'sleeping',\n",
              " 493: 'read',\n",
              " 494: 'news',\n",
              " 495: 'ma',\n",
              " 496: 'belle',\n",
              " 497: 'everyone',\n",
              " 498: 'write',\n",
              " 499: 'roll',\n",
              " 500: 'off',\n",
              " 501: 'looked',\n",
              " 502: 'working',\n",
              " 503: 'hands',\n",
              " 504: 'held',\n",
              " 505: \"day's\",\n",
              " 506: 'lot',\n",
              " 507: 'tells',\n",
              " 508: 'wants',\n",
              " 509: 'knowing',\n",
              " 510: 'someday',\n",
              " 511: 'listen',\n",
              " 512: 'shade',\n",
              " 513: 'anymore',\n",
              " 514: 'nearly',\n",
              " 515: 'broke',\n",
              " 516: 'ha',\n",
              " 517: 'plans',\n",
              " 518: 'maxwell',\n",
              " 519: 'tax',\n",
              " 520: 'suddenly',\n",
              " 521: 'game',\n",
              " 522: 'myself',\n",
              " 523: 'shine',\n",
              " 524: 'om',\n",
              " 525: 'inside',\n",
              " 526: 'ringing',\n",
              " 527: 'calls',\n",
              " 528: 'begin',\n",
              " 529: 'pain',\n",
              " 530: 'plays',\n",
              " 531: 'feels',\n",
              " 532: 'slowly',\n",
              " 533: 'slumbers',\n",
              " 534: 'rise',\n",
              " 535: 'equal',\n",
              " 536: 'everybody',\n",
              " 537: 'getting',\n",
              " 538: 'da',\n",
              " 539: 'gives',\n",
              " 540: 'built',\n",
              " 541: 'running',\n",
              " 542: 'lend',\n",
              " 543: 'evening',\n",
              " 544: 'sending',\n",
              " 545: 'lights',\n",
              " 546: 'sunday',\n",
              " 547: 'summer',\n",
              " 548: 'rent',\n",
              " 549: 'drop',\n",
              " 550: 'touch',\n",
              " 551: 'guess',\n",
              " 552: 'appreciate',\n",
              " 553: 'being',\n",
              " 554: 'lover',\n",
              " 555: 'shows',\n",
              " 556: 'grow',\n",
              " 557: 'below',\n",
              " 558: 'wonderful',\n",
              " 559: \"we'd\",\n",
              " 560: 'pictures',\n",
              " 561: 'lived',\n",
              " 562: 'land',\n",
              " 563: 'full',\n",
              " 564: 'speed',\n",
              " 565: 'mr',\n",
              " 566: 'sir',\n",
              " 567: 'name',\n",
              " 568: 'loud',\n",
              " 569: 'walk',\n",
              " 570: 'row',\n",
              " 571: 'english',\n",
              " 572: 'hee',\n",
              " 573: 'hah',\n",
              " 574: 'father',\n",
              " 575: 'notice',\n",
              " 576: 'house',\n",
              " 577: 'holes',\n",
              " 578: 'small',\n",
              " 579: 'sont',\n",
              " 580: 'les',\n",
              " 581: 'mots',\n",
              " 582: 'qui',\n",
              " 583: 'vont',\n",
              " 584: 'hoping',\n",
              " 585: 'barber',\n",
              " 586: 'banker',\n",
              " 587: 'suburban',\n",
              " 588: 'meanwhile',\n",
              " 589: 'keep',\n",
              " 590: 'jojo',\n",
              " 591: 'cha',\n",
              " 592: 'boat',\n",
              " 593: 'junior',\n",
              " 594: 'behave',\n",
              " 595: 'lost',\n",
              " 596: \"you'd\",\n",
              " 597: 'ooh',\n",
              " 598: 'morning',\n",
              " 599: 'honey',\n",
              " 600: 'fine',\n",
              " 601: 'woo',\n",
              " 602: 'hurt',\n",
              " 603: 'belong',\n",
              " 604: 'writing',\n",
              " 605: 'died',\n",
              " 606: 'fall',\n",
              " 607: 'missing',\n",
              " 608: 'dog',\n",
              " 609: 'earth',\n",
              " 610: 'ok',\n",
              " 611: 'holding',\n",
              " 612: 'diamond',\n",
              " 613: 'anything',\n",
              " 614: 'ago',\n",
              " 615: \"we've\",\n",
              " 616: 'wonder',\n",
              " 617: 'winding',\n",
              " 618: 'cried',\n",
              " 619: 'tried',\n",
              " 620: 'young',\n",
              " 621: 'ten',\n",
              " 622: 'e',\n",
              " 623: 'measured',\n",
              " 624: 'quiet',\n",
              " 625: 'knock',\n",
              " 626: 'pay',\n",
              " 627: 'monday',\n",
              " 628: 'soon',\n",
              " 629: 'king',\n",
              " 630: 'pam',\n",
              " 631: 'dirty',\n",
              " 632: 'polythene',\n",
              " 633: \"majesty's\",\n",
              " 634: 'nice',\n",
              " 635: 'realized',\n",
              " 636: 'raccoon',\n",
              " 637: 'even',\n",
              " 638: 'looks',\n",
              " 639: 'mother',\n",
              " 640: 'mary',\n",
              " 641: 'agree',\n",
              " 642: 'shines',\n",
              " 643: 'sound',\n",
              " 644: 'cup',\n",
              " 645: 'joy',\n",
              " 646: 'which',\n",
              " 647: 'million',\n",
              " 648: 'call',\n",
              " 649: 'sounds',\n",
              " 650: 'skin',\n",
              " 651: 'cool',\n",
              " 652: 'remain',\n",
              " 653: 'moments',\n",
              " 654: 'recall',\n",
              " 655: 'compares',\n",
              " 656: 'memories',\n",
              " 657: 'meaning',\n",
              " 658: 'wings',\n",
              " 659: 'cold',\n",
              " 660: 'awake',\n",
              " 661: 'pillow',\n",
              " 662: 'invitations',\n",
              " 663: 'celebrations',\n",
              " 664: 'saved',\n",
              " 665: 'works',\n",
              " 666: 'matter',\n",
              " 667: 'tune',\n",
              " 668: 'singer',\n",
              " 669: 'bra',\n",
              " 670: 'twenty',\n",
              " 671: 'begins',\n",
              " 672: 'kids',\n",
              " 673: 'yard',\n",
              " 674: 'jones',\n",
              " 675: 'market',\n",
              " 676: 'lets',\n",
              " 677: 'stays',\n",
              " 678: 'losing',\n",
              " 679: 'valentine',\n",
              " 680: 'greetings',\n",
              " 681: 'bottle',\n",
              " 682: 'quarter',\n",
              " 683: 'lock',\n",
              " 684: 'word',\n",
              " 685: 'handy',\n",
              " 686: 'mending',\n",
              " 687: 'fuse',\n",
              " 688: 'knit',\n",
              " 689: 'sweater',\n",
              " 690: 'fireside',\n",
              " 691: 'ride',\n",
              " 692: 'digging',\n",
              " 693: 'weeds',\n",
              " 694: 'cottage',\n",
              " 695: 'isle',\n",
              " 696: 'wight',\n",
              " 697: 'shall',\n",
              " 698: 'scrimp',\n",
              " 699: 'grandchildren',\n",
              " 700: 'vera',\n",
              " 701: 'chuck',\n",
              " 702: 'dave',\n",
              " 703: 'postcard',\n",
              " 704: 'line',\n",
              " 705: 'stating',\n",
              " 706: 'precisely',\n",
              " 707: 'sincerely',\n",
              " 708: 'wasting',\n",
              " 709: 'form',\n",
              " 710: \"feelin'\",\n",
              " 711: 'past',\n",
              " 712: 'needed',\n",
              " 713: \"anybody's\",\n",
              " 714: 'self',\n",
              " 715: 'assured',\n",
              " 716: 'doors',\n",
              " 717: 'ways',\n",
              " 718: 'somewhere',\n",
              " 719: 'style',\n",
              " 720: 'flat',\n",
              " 721: 'joo',\n",
              " 722: 'roller',\n",
              " 723: 'joker',\n",
              " 724: 'finger',\n",
              " 725: 'shoot',\n",
              " 726: 'early',\n",
              " 727: 'birds',\n",
              " 728: 'roses',\n",
              " 729: 'fragrant',\n",
              " 730: 'meadows',\n",
              " 731: 'dawn',\n",
              " 732: 'dew',\n",
              " 733: 'count',\n",
              " 734: 'brother',\n",
              " 735: 'wait',\n",
              " 736: 'town',\n",
              " 737: 'sailed',\n",
              " 738: 'next',\n",
              " 739: 'cut',\n",
              " 740: 'cable',\n",
              " 741: 'aye',\n",
              " 742: 'captain',\n",
              " 743: 'whenever',\n",
              " 744: 'air',\n",
              " 745: 'endear',\n",
              " 746: 'worry',\n",
              " 747: 'own',\n",
              " 748: 'sight',\n",
              " 749: 'pigs',\n",
              " 750: 'gun',\n",
              " 751: 'stupid',\n",
              " 752: 'naughty',\n",
              " 753: 'eye',\n",
              " 754: 'set',\n",
              " 755: 'turning',\n",
              " 756: 'rather',\n",
              " 757: 'car',\n",
              " 758: 'crowd',\n",
              " 759: 'book',\n",
              " 760: 'noticed',\n",
              " 761: 'late',\n",
              " 762: 'thousand',\n",
              " 763: 'pleasure',\n",
              " 764: 'corner',\n",
              " 765: 'pouring',\n",
              " 766: 'strange',\n",
              " 767: 'fireman',\n",
              " 768: 'queen',\n",
              " 769: 'fire',\n",
              " 770: 'clean',\n",
              " 771: 'pies',\n",
              " 772: 'shaves',\n",
              " 773: 'loretta',\n",
              " 774: 'woman',\n",
              " 775: 'coming',\n",
              " 776: 'gets',\n",
              " 777: 'turns',\n",
              " 778: 'picture',\n",
              " 779: 'quite',\n",
              " 780: 'kaleidoscope',\n",
              " 781: 'flowers',\n",
              " 782: 'rocking',\n",
              " 783: 'eat',\n",
              " 784: 'appear',\n",
              " 785: 'clouds',\n",
              " 786: 'school',\n",
              " 787: 'chair',\n",
              " 788: 'puts',\n",
              " 789: 'teacher',\n",
              " 790: 'ready',\n",
              " 791: \"junior's\",\n",
              " 792: 'shop',\n",
              " 793: 'bath',\n",
              " 794: 'brings',\n",
              " 795: 'bright',\n",
              " 796: 'stars',\n",
              " 797: 'bye',\n",
              " 798: 'wow',\n",
              " 799: 'norwegian',\n",
              " 800: 'wood',\n",
              " 801: 'worked',\n",
              " 802: 'closer',\n",
              " 803: 'yi',\n",
              " 804: 'yay',\n",
              " 805: 'thinking',\n",
              " 806: 'pride',\n",
              " 807: 'eleanor',\n",
              " 808: 'rigby',\n",
              " 809: 'church',\n",
              " 810: 'waits',\n",
              " 811: 'window',\n",
              " 812: 'wearing',\n",
              " 813: 'mckenzie',\n",
              " 814: 'along',\n",
              " 815: 'boom',\n",
              " 816: 'crossed',\n",
              " 817: 'danced',\n",
              " 818: 'close',\n",
              " 819: 'miss',\n",
              " 820: 'lips',\n",
              " 821: 'log',\n",
              " 822: 'moan',\n",
              " 823: 'owww',\n",
              " 824: 'special',\n",
              " 825: 'sunny',\n",
              " 826: 'lie',\n",
              " 827: 'satisfied',\n",
              " 828: 'introduce',\n",
              " 829: 'might',\n",
              " 830: 'beside',\n",
              " 831: 'share',\n",
              " 832: 'believing',\n",
              " 833: 'dies',\n",
              " 834: 'watching',\n",
              " 835: 'forget',\n",
              " 836: \"di'n'di\",\n",
              " 837: 'bup',\n",
              " 838: 'dreamer',\n",
              " 839: 'join',\n",
              " 840: 'leads',\n",
              " 841: 'enough',\n",
              " 842: 'did',\n",
              " 843: 'warm',\n",
              " 844: 'lies',\n",
              " 845: 'safe',\n",
              " 846: 'c',\n",
              " 847: 'sail',\n",
              " 848: 'ship',\n",
              " 849: 'chop',\n",
              " 850: 'skip',\n",
              " 851: 'rope',\n",
              " 852: 'celebrate',\n",
              " 853: 'penetrate',\n",
              " 854: 'pick',\n",
              " 855: 'radiate',\n",
              " 856: 'imitate',\n",
              " 857: 'lorry',\n",
              " 858: 'syndicate',\n",
              " 859: \"haven't\",\n",
              " 860: 'putting',\n",
              " 861: 'joke',\n",
              " 862: 'brain',\n",
              " 863: 'weeks',\n",
              " 864: 'insane',\n",
              " 865: 'park',\n",
              " 866: 'roof',\n",
              " 867: 'bag',\n",
              " 868: 'ukraine',\n",
              " 869: 'west',\n",
              " 870: 'moscow',\n",
              " 871: \"georgia's\",\n",
              " 872: 'hu',\n",
              " 873: 'mountain',\n",
              " 874: 'grin',\n",
              " 875: 'perfectly',\n",
              " 876: 'talking',\n",
              " 877: 'magic',\n",
              " 878: 'na',\n",
              " 879: \"everybody's\",\n",
              " 880: 'mi',\n",
              " 881: 'amore',\n",
              " 882: 'sleeps',\n",
              " 883: 'dressed',\n",
              " 884: \"sunday's\",\n",
              " 885: \"tuesday's\",\n",
              " 886: 'wall',\n",
              " 887: 'greet',\n",
              " 888: 'brand',\n",
              " 889: 'beautiful',\n",
              " 890: 'open',\n",
              " 891: 'happen',\n",
              " 892: 'bit',\n",
              " 893: 'command',\n",
              " 894: 'himself',\n",
              " 895: 'checked',\n",
              " 896: \"gideon's\",\n",
              " 897: 'bible',\n",
              " 898: 'rival',\n",
              " 899: 'called',\n",
              " 900: 'doc',\n",
              " 901: 'saying',\n",
              " 902: 'short',\n",
              " 903: 'fussing',\n",
              " 904: 'fighting',\n",
              " 905: 'crime',\n",
              " 906: 'oa',\n",
              " 907: 'troubles',\n",
              " 908: 'seemed',\n",
              " 909: 'far',\n",
              " 910: 'half',\n",
              " 911: 'used',\n",
              " 912: 'shadow',\n",
              " 913: 'hanging',\n",
              " 914: 'trouble',\n",
              " 915: 'hour',\n",
              " 916: 'darkness',\n",
              " 917: 'front',\n",
              " 918: 'hearted',\n",
              " 919: 'parted',\n",
              " 920: 'cloudy',\n",
              " 921: 'wake',\n",
              " 922: 'flowing',\n",
              " 923: 'endless',\n",
              " 924: 'slither',\n",
              " 925: 'wildly',\n",
              " 926: 'slip',\n",
              " 927: 'pools',\n",
              " 928: 'sorrow',\n",
              " 929: 'drifting',\n",
              " 930: 'possessing',\n",
              " 931: 'caressing',\n",
              " 932: 'images',\n",
              " 933: 'thoughts',\n",
              " 934: 'meander',\n",
              " 935: 'restless',\n",
              " 936: 'letter',\n",
              " 937: 'box',\n",
              " 938: 'tumble',\n",
              " 939: 'blindly',\n",
              " 940: 'laughter',\n",
              " 941: 'shades',\n",
              " 942: 'inciting',\n",
              " 943: 'inviting',\n",
              " 944: 'limitless',\n",
              " 945: 'undying',\n",
              " 946: 'suns',\n",
              " 947: 'afraid',\n",
              " 948: 'minute',\n",
              " 949: 'anytime',\n",
              " 950: 'refrain',\n",
              " 951: 'shoulders',\n",
              " 952: 'colder',\n",
              " 953: 'perform',\n",
              " 954: 'movement',\n",
              " 955: 'shoulder',\n",
              " 956: 'sunken',\n",
              " 957: 'winter',\n",
              " 958: 'returning',\n",
              " 959: 'faces',\n",
              " 960: 'ice',\n",
              " 961: 'melting',\n",
              " 962: 'clear',\n",
              " 963: 'sung',\n",
              " 964: 'shown',\n",
              " 965: 'meant',\n",
              " 966: \"mem'ries\",\n",
              " 967: 'closed',\n",
              " 968: 'misunderstanding',\n",
              " 969: 'sometimes',\n",
              " 970: 'disagree',\n",
              " 971: 'barrow',\n",
              " 972: 'marketplace',\n",
              " 973: 'trolley',\n",
              " 974: \"jeweler's\",\n",
              " 975: 'store',\n",
              " 976: 'buys',\n",
              " 977: 'carat',\n",
              " 978: 'sings',\n",
              " 979: 'fun',\n",
              " 980: 'bla',\n",
              " 981: \"'til\",\n",
              " 982: 'mornings',\n",
              " 983: 'yours',\n",
              " 984: 'lasts',\n",
              " 985: 'independence',\n",
              " 986: 'vanish',\n",
              " 987: 'haze',\n",
              " 988: 'insecure',\n",
              " 989: 'moves',\n",
              " 990: 'attracts',\n",
              " 991: 'woos',\n",
              " 992: 'asking',\n",
              " 993: 'stick',\n",
              " 994: 'top',\n",
              " 995: \"groovin'\",\n",
              " 996: 'eyeballs',\n",
              " 997: 'holy',\n",
              " 998: 'wear',\n",
              " 999: 'shoeshine',\n",
              " 1000: 'toe',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJgVhq1zwEpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe46d65-f92c-4bd7-f45d-b69d58cc8831"
      },
      "source": [
        "# Cantidad de palabras en el vocabulario\n",
        "vocab_size = len(tok.word_counts)\n",
        "vocab_size"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1628"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWjNrNx9wM1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c78633e-2476-4eab-8e55-21bce473690e"
      },
      "source": [
        "# ¡Ojo! y_data_int comienza en \"1\" en vez de \"0\"\n",
        "# valor minimo:\n",
        "min(y_data_int)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIg2e2WCwXbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b706269-542c-40a0-d933-f7ec644de029"
      },
      "source": [
        "# Hay que restar 1\n",
        "y_data_int_offset = y_data_int - 1\n",
        "min(y_data_int_offset)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        # Convertir los arrays de numpy a tensores.\n",
        "        # pytorch espera en general entradas 32bits\n",
        "        self.x = torch.from_numpy(x.astype(np.int32))\n",
        "        # Transformar los datos a oneHotEncoding\n",
        "        # la loss function esperan la salida float\n",
        "        self.y = F.one_hot(torch.from_numpy(y), num_classes=vocab_size).float()\n",
        "\n",
        "        self.len = self.y.shape[0]\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "data_set = Data(x_data, y_data_int_offset)\n",
        "\n",
        "input_size = data_set.x.shape[1]\n",
        "print(\"input_size:\", input_size)\n",
        "\n",
        "output_dim = data_set.y.shape[1]\n",
        "print(\"Output dim\", output_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVO59n_3BEyR",
        "outputId": "2a22f0f4-b88c-481d-d164-e6cbd7f71021"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_size: 3\n",
            "Output dim 1628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "valid_set_size = int(data_set.len * 0.2)\n",
        "train_set_size = data_set.len - valid_set_size\n",
        "\n",
        "train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n",
        "valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len))\n",
        "\n",
        "print(\"Tamaño del conjunto de entrenamiento:\", len(train_set))\n",
        "print(\"Tamaño del conjunto de validacion:\", len(valid_set))\n",
        "\n",
        "train_batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_HzwNSwCISC",
        "outputId": "8c17f824-ce3d-4fbd-e12d-88b35d151600"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del conjunto de entrenamiento: 11507\n",
            "Tamaño del conjunto de validacion: 2876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmJWNyxQwfCE"
      },
      "source": [
        "### 4 - Entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model1(nn.Module):\n",
        "    def __init__(self, vocab_size, output_dim):\n",
        "        super().__init__()\n",
        "        # num_embeddings = vocab_size --> 1628 palabras distintas + 1 para padding o UNK\n",
        "        # embedding_dim = 5 --> crear embeddings de tamaño 5 (tamaño variable y ajustable)\n",
        "        self.lstm_size = 64\n",
        "        self.num_layers = 2\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size+1, embedding_dim=5, padding_idx=0)\n",
        "        self.lstm1 = nn.LSTM(input_size=5, hidden_size=self.lstm_size, batch_first=True,\n",
        "                            num_layers=self.num_layers, dropout=0.2) # LSTM layer\n",
        "        self.fc1 = nn.Linear(in_features=self.lstm_size, out_features=32) # Fully connected layer\n",
        "        self.fc2 = nn.Linear(in_features=32, out_features=output_dim) # Fully connected layer\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1) # normalize in dim 1\n",
        "\n",
        "    def forward(self, x, prev_state=None):\n",
        "        if prev_state is None:\n",
        "            # En cada nueva inferencia reinicio el hidden state\n",
        "            # de la LSTM al menos que sea pasado por parámetro el\n",
        "            # elstado de previo\n",
        "            # Esta acción se realiza especialmente para que\n",
        "            # el hidden_state de la última inferencia no afecte\n",
        "            # a la siguiente\n",
        "            batch_size = x.shape[0] #(batch, seq_size)\n",
        "            prev_state = self.init_hidden(batch_size)\n",
        "\n",
        "        out = self.embedding(x)\n",
        "        lstm_output, (ht, ct) = self.lstm1(out, prev_state)\n",
        "        out = self.relu(self.fc1(lstm_output[:,-1,:])) # take last output (last seq)\n",
        "        out = self.softmax(self.fc2(out))\n",
        "        return out\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.lstm_size),\n",
        "                torch.zeros(self.num_layers, batch_size, self.lstm_size))\n",
        "\n",
        "model1 = Model1(vocab_size=vocab_size, output_dim=output_dim)\n",
        "\n",
        "# Crear el optimizador la una función de error\n",
        "model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
        "model1_criterion = torch.nn.CrossEntropyLoss()  # Para clasificación multi categórica\n",
        "\n",
        "# Por defecto torchinfo testea el modelo con torch.FloatTensor\n",
        "summary(model1, input_size=(1, input_size), dtypes=['torch.IntTensor'], device=torch.device('cpu'))\n",
        "\n",
        "# otra posibilidad\n",
        "#summary(model1, input_data=data_set[0][0].unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsdYibawCkHi",
        "outputId": "d0c9ff81-5827-4144-c5ab-61ecfbf62c66"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Model1                                   [1, 1628]                 --\n",
              "├─Embedding: 1-1                         [1, 3, 5]                 8,145\n",
              "├─LSTM: 1-2                              [1, 3, 64]                51,456\n",
              "├─Linear: 1-3                            [1, 32]                   2,080\n",
              "├─ReLU: 1-4                              [1, 32]                   --\n",
              "├─Linear: 1-5                            [1, 1628]                 53,724\n",
              "├─Softmax: 1-6                           [1, 1628]                 --\n",
              "==========================================================================================\n",
              "Total params: 115,405\n",
              "Trainable params: 115,405\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.22\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.01\n",
              "Params size (MB): 0.46\n",
              "Estimated Total Size (MB): 0.48\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history1 = train(model1,\n",
        "                train_loader,\n",
        "                valid_loader,\n",
        "                model1_optimizer,\n",
        "                model1_criterion,\n",
        "                epochs=100\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5yINRb7yMzB",
        "outputId": "95c4bc44-77cc-4265-886d-7edbd5727464"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100 - Train loss 7.356 - Train accuracy 0.045 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 2/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 3/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 4/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 5/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 6/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 7/100 - Train loss 7.346 - Train accuracy 0.051 - Valid Loss 7.354 - Valid accuracy 0.042\n",
            "Epoch: 8/100 - Train loss 7.344 - Train accuracy 0.052 - Valid Loss 7.354 - Valid accuracy 0.042\n",
            "Epoch: 9/100 - Train loss 7.343 - Train accuracy 0.053 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 10/100 - Train loss 7.344 - Train accuracy 0.052 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 11/100 - Train loss 7.337 - Train accuracy 0.059 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 12/100 - Train loss 7.334 - Train accuracy 0.062 - Valid Loss 7.357 - Valid accuracy 0.040\n",
            "Epoch: 13/100 - Train loss 7.333 - Train accuracy 0.063 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 14/100 - Train loss 7.334 - Train accuracy 0.063 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 15/100 - Train loss 7.334 - Train accuracy 0.062 - Valid Loss 7.357 - Valid accuracy 0.040\n",
            "Epoch: 16/100 - Train loss 7.333 - Train accuracy 0.063 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 17/100 - Train loss 7.333 - Train accuracy 0.063 - Valid Loss 7.357 - Valid accuracy 0.040\n",
            "Epoch: 18/100 - Train loss 7.333 - Train accuracy 0.063 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 19/100 - Train loss 7.333 - Train accuracy 0.063 - Valid Loss 7.357 - Valid accuracy 0.040\n",
            "Epoch: 20/100 - Train loss 7.333 - Train accuracy 0.063 - Valid Loss 7.356 - Valid accuracy 0.041\n",
            "Epoch: 21/100 - Train loss 7.333 - Train accuracy 0.063 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 22/100 - Train loss 7.333 - Train accuracy 0.063 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 23/100 - Train loss 7.333 - Train accuracy 0.063 - Valid Loss 7.355 - Valid accuracy 0.041\n",
            "Epoch: 24/100 - Train loss 7.333 - Train accuracy 0.063 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 25/100 - Train loss 7.333 - Train accuracy 0.063 - Valid Loss 7.356 - Valid accuracy 0.041\n",
            "Epoch: 26/100 - Train loss 7.333 - Train accuracy 0.063 - Valid Loss 7.357 - Valid accuracy 0.040\n",
            "Epoch: 27/100 - Train loss 7.333 - Train accuracy 0.063 - Valid Loss 7.358 - Valid accuracy 0.039\n",
            "Epoch: 28/100 - Train loss 7.336 - Train accuracy 0.060 - Valid Loss 7.358 - Valid accuracy 0.039\n",
            "Epoch: 29/100 - Train loss 7.336 - Train accuracy 0.060 - Valid Loss 7.358 - Valid accuracy 0.039\n",
            "Epoch: 30/100 - Train loss 7.335 - Train accuracy 0.061 - Valid Loss 7.360 - Valid accuracy 0.036\n",
            "Epoch: 31/100 - Train loss 7.335 - Train accuracy 0.061 - Valid Loss 7.356 - Valid accuracy 0.039\n",
            "Epoch: 32/100 - Train loss 7.334 - Train accuracy 0.062 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 33/100 - Train loss 7.333 - Train accuracy 0.063 - Valid Loss 7.354 - Valid accuracy 0.043\n",
            "Epoch: 34/100 - Train loss 7.332 - Train accuracy 0.064 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 35/100 - Train loss 7.332 - Train accuracy 0.064 - Valid Loss 7.355 - Valid accuracy 0.041\n",
            "Epoch: 36/100 - Train loss 7.331 - Train accuracy 0.065 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 37/100 - Train loss 7.331 - Train accuracy 0.065 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 38/100 - Train loss 7.331 - Train accuracy 0.065 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 39/100 - Train loss 7.331 - Train accuracy 0.065 - Valid Loss 7.355 - Valid accuracy 0.041\n",
            "Epoch: 40/100 - Train loss 7.332 - Train accuracy 0.064 - Valid Loss 7.355 - Valid accuracy 0.041\n",
            "Epoch: 41/100 - Train loss 7.332 - Train accuracy 0.064 - Valid Loss 7.355 - Valid accuracy 0.041\n",
            "Epoch: 42/100 - Train loss 7.332 - Train accuracy 0.064 - Valid Loss 7.353 - Valid accuracy 0.043\n",
            "Epoch: 43/100 - Train loss 7.332 - Train accuracy 0.064 - Valid Loss 7.353 - Valid accuracy 0.043\n",
            "Epoch: 44/100 - Train loss 7.331 - Train accuracy 0.065 - Valid Loss 7.352 - Valid accuracy 0.044\n",
            "Epoch: 45/100 - Train loss 7.331 - Train accuracy 0.065 - Valid Loss 7.355 - Valid accuracy 0.041\n",
            "Epoch: 46/100 - Train loss 7.331 - Train accuracy 0.065 - Valid Loss 7.355 - Valid accuracy 0.041\n",
            "Epoch: 47/100 - Train loss 7.329 - Train accuracy 0.067 - Valid Loss 7.354 - Valid accuracy 0.041\n",
            "Epoch: 48/100 - Train loss 7.328 - Train accuracy 0.068 - Valid Loss 7.354 - Valid accuracy 0.042\n",
            "Epoch: 49/100 - Train loss 7.328 - Train accuracy 0.069 - Valid Loss 7.353 - Valid accuracy 0.043\n",
            "Epoch: 50/100 - Train loss 7.328 - Train accuracy 0.069 - Valid Loss 7.354 - Valid accuracy 0.042\n",
            "Epoch: 51/100 - Train loss 7.327 - Train accuracy 0.069 - Valid Loss 7.353 - Valid accuracy 0.043\n",
            "Epoch: 52/100 - Train loss 7.329 - Train accuracy 0.067 - Valid Loss 7.354 - Valid accuracy 0.043\n",
            "Epoch: 53/100 - Train loss 7.331 - Train accuracy 0.065 - Valid Loss 7.351 - Valid accuracy 0.046\n",
            "Epoch: 54/100 - Train loss 7.331 - Train accuracy 0.065 - Valid Loss 7.352 - Valid accuracy 0.045\n",
            "Epoch: 55/100 - Train loss 7.327 - Train accuracy 0.069 - Valid Loss 7.354 - Valid accuracy 0.042\n",
            "Epoch: 56/100 - Train loss 7.326 - Train accuracy 0.070 - Valid Loss 7.352 - Valid accuracy 0.045\n",
            "Epoch: 57/100 - Train loss 7.325 - Train accuracy 0.071 - Valid Loss 7.352 - Valid accuracy 0.044\n",
            "Epoch: 58/100 - Train loss 7.324 - Train accuracy 0.072 - Valid Loss 7.352 - Valid accuracy 0.044\n",
            "Epoch: 59/100 - Train loss 7.325 - Train accuracy 0.072 - Valid Loss 7.352 - Valid accuracy 0.043\n",
            "Epoch: 60/100 - Train loss 7.324 - Train accuracy 0.072 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 61/100 - Train loss 7.325 - Train accuracy 0.071 - Valid Loss 7.352 - Valid accuracy 0.044\n",
            "Epoch: 62/100 - Train loss 7.325 - Train accuracy 0.072 - Valid Loss 7.352 - Valid accuracy 0.044\n",
            "Epoch: 63/100 - Train loss 7.325 - Train accuracy 0.072 - Valid Loss 7.352 - Valid accuracy 0.045\n",
            "Epoch: 64/100 - Train loss 7.324 - Train accuracy 0.072 - Valid Loss 7.352 - Valid accuracy 0.045\n",
            "Epoch: 65/100 - Train loss 7.323 - Train accuracy 0.073 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 66/100 - Train loss 7.324 - Train accuracy 0.072 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 67/100 - Train loss 7.324 - Train accuracy 0.072 - Valid Loss 7.352 - Valid accuracy 0.043\n",
            "Epoch: 68/100 - Train loss 7.322 - Train accuracy 0.074 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 69/100 - Train loss 7.322 - Train accuracy 0.075 - Valid Loss 7.353 - Valid accuracy 0.043\n",
            "Epoch: 70/100 - Train loss 7.322 - Train accuracy 0.074 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 71/100 - Train loss 7.322 - Train accuracy 0.074 - Valid Loss 7.349 - Valid accuracy 0.048\n",
            "Epoch: 72/100 - Train loss 7.321 - Train accuracy 0.075 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 73/100 - Train loss 7.321 - Train accuracy 0.076 - Valid Loss 7.353 - Valid accuracy 0.043\n",
            "Epoch: 74/100 - Train loss 7.321 - Train accuracy 0.076 - Valid Loss 7.351 - Valid accuracy 0.046\n",
            "Epoch: 75/100 - Train loss 7.320 - Train accuracy 0.076 - Valid Loss 7.349 - Valid accuracy 0.047\n",
            "Epoch: 76/100 - Train loss 7.319 - Train accuracy 0.078 - Valid Loss 7.350 - Valid accuracy 0.047\n",
            "Epoch: 77/100 - Train loss 7.319 - Train accuracy 0.078 - Valid Loss 7.349 - Valid accuracy 0.047\n",
            "Epoch: 78/100 - Train loss 7.318 - Train accuracy 0.078 - Valid Loss 7.349 - Valid accuracy 0.047\n",
            "Epoch: 79/100 - Train loss 7.317 - Train accuracy 0.079 - Valid Loss 7.350 - Valid accuracy 0.046\n",
            "Epoch: 80/100 - Train loss 7.316 - Train accuracy 0.080 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 81/100 - Train loss 7.317 - Train accuracy 0.079 - Valid Loss 7.352 - Valid accuracy 0.045\n",
            "Epoch: 82/100 - Train loss 7.317 - Train accuracy 0.080 - Valid Loss 7.349 - Valid accuracy 0.047\n",
            "Epoch: 83/100 - Train loss 7.317 - Train accuracy 0.080 - Valid Loss 7.350 - Valid accuracy 0.047\n",
            "Epoch: 84/100 - Train loss 7.316 - Train accuracy 0.080 - Valid Loss 7.350 - Valid accuracy 0.046\n",
            "Epoch: 85/100 - Train loss 7.316 - Train accuracy 0.080 - Valid Loss 7.350 - Valid accuracy 0.046\n",
            "Epoch: 86/100 - Train loss 7.316 - Train accuracy 0.081 - Valid Loss 7.349 - Valid accuracy 0.047\n",
            "Epoch: 87/100 - Train loss 7.318 - Train accuracy 0.079 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 88/100 - Train loss 7.317 - Train accuracy 0.079 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 89/100 - Train loss 7.317 - Train accuracy 0.080 - Valid Loss 7.350 - Valid accuracy 0.047\n",
            "Epoch: 90/100 - Train loss 7.316 - Train accuracy 0.081 - Valid Loss 7.349 - Valid accuracy 0.047\n",
            "Epoch: 91/100 - Train loss 7.315 - Train accuracy 0.081 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 92/100 - Train loss 7.315 - Train accuracy 0.082 - Valid Loss 7.350 - Valid accuracy 0.046\n",
            "Epoch: 93/100 - Train loss 7.314 - Train accuracy 0.082 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 94/100 - Train loss 7.314 - Train accuracy 0.083 - Valid Loss 7.352 - Valid accuracy 0.045\n",
            "Epoch: 95/100 - Train loss 7.315 - Train accuracy 0.081 - Valid Loss 7.354 - Valid accuracy 0.042\n",
            "Epoch: 96/100 - Train loss 7.314 - Train accuracy 0.082 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 97/100 - Train loss 7.313 - Train accuracy 0.084 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 98/100 - Train loss 7.313 - Train accuracy 0.084 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 99/100 - Train loss 7.313 - Train accuracy 0.083 - Valid Loss 7.351 - Valid accuracy 0.045\n",
            "Epoch: 100/100 - Train loss 7.313 - Train accuracy 0.083 - Valid Loss 7.350 - Valid accuracy 0.046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#Aumento del Embedding Dim: Incrementamos el tamaño del embedding a 128 para capturar más información semántica.\n",
        "#Capas Convolucionales: Se agregaron dos capas Conv1D con MaxPooling para extraer características locales antes de alimentar los datos a la LSTM.\n",
        "#Bidireccional LSTM: Convertimos la LSTM en bidireccional para capturar dependencias en ambas direcciones de la secuencia.\n",
        "#Capas LSTM: Incrementamos el número de capas LSTM de 2 a 3, y el tamaño del hidden state de 64 a 256.\n",
        "#Capas Fully Connected: Se añadió una capa adicional con 64 unidades y se incrementó la primera capa a 128 unidades.\n",
        "#Dropout: Se añadió Dropout después de la primera capa totalmente conectada para reducir el riesgo de overfitting.\n",
        "\n",
        "\n",
        "\n",
        "class Model2(nn.Module):\n",
        "    def __init__(self, vocab_size, output_dim):\n",
        "        super().__init__()\n",
        "        # Incrementamos el tamaño del embedding y las dimensiones de la LSTM\n",
        "        self.embedding_dim = 128  # Aumentamos el tamaño de los embeddings\n",
        "        self.lstm_size = 256  # Aumentamos el tamaño de los hidden states\n",
        "        self.num_layers = 3  # Aumentamos el número de capas LSTM\n",
        "\n",
        "        # Embedding Layer\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size+1, embedding_dim=self.embedding_dim, padding_idx=0)\n",
        "\n",
        "        # Añadimos una capa convolucional antes de la LSTM\n",
        "        self.conv1 = nn.Conv1d(in_channels=self.embedding_dim, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # LSTM Layer\n",
        "        self.lstm1 = nn.LSTM(input_size=256, hidden_size=self.lstm_size, batch_first=True,\n",
        "                             num_layers=self.num_layers, dropout=0.3, bidirectional=True)  # Bidirectional LSTM\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(in_features=self.lstm_size * 2, out_features=128)  # Doble tamaño por la LSTM bidireccional\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=64)  # Capa adicional\n",
        "        self.fc3 = nn.Linear(in_features=64, out_features=output_dim)\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x, prev_state=None):\n",
        "        if prev_state is None:\n",
        "            batch_size = x.shape[0]\n",
        "            prev_state = self.init_hidden(batch_size)\n",
        "\n",
        "        out = self.embedding(x)\n",
        "\n",
        "        # Conv1D layers\n",
        "        out = self.conv1(out.transpose(1, 2))  # Conv1D espera (batch_size, embedding_dim, sequence_length)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.pool(out)\n",
        "\n",
        "        out = out.transpose(1, 2)  # Volvemos a (batch_size, sequence_length, conv_output_dim)\n",
        "\n",
        "        # LSTM layers\n",
        "        lstm_output, (ht, ct) = self.lstm1(out, prev_state)\n",
        "\n",
        "        # Fully Connected layers\n",
        "        out = self.relu(self.fc1(lstm_output[:, -1, :]))  # Consideramos la última salida\n",
        "        out = self.dropout(out)  # Dropout para evitar overfitting\n",
        "        out = self.relu(self.fc2(out))  # Capa adicional\n",
        "        out = self.fc3(out)\n",
        "        out = self.softmax(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Inicializamos los hidden states para una LSTM bidireccional\n",
        "        return (torch.zeros(self.num_layers * 2, batch_size, self.lstm_size),\n",
        "                torch.zeros(self.num_layers * 2, batch_size, self.lstm_size))\n",
        "\n",
        "model2 = Model2(vocab_size=vocab_size, output_dim=output_dim)\n",
        "\n",
        "# Crear el optimizador y la función de error\n",
        "model2_optimizer = torch.optim.Adam(model2.parameters(), lr=0.0001)\n",
        "model2_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Resumen del modelo\n",
        "summary(model2, input_size=(1, input_size), dtypes=['torch.IntTensor'], device=torch.device('cpu'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2sJbtKfq6Sr",
        "outputId": "ad6d5468-20fd-417b-d504-2decaafa2edf"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Model2                                   [1, 1628]                 --\n",
              "├─Embedding: 1-1                         [1, 3, 128]               208,512\n",
              "├─Conv1d: 1-2                            [1, 128, 3]               49,280\n",
              "├─ReLU: 1-3                              [1, 128, 3]               --\n",
              "├─Conv1d: 1-4                            [1, 256, 3]               98,560\n",
              "├─ReLU: 1-5                              [1, 256, 3]               --\n",
              "├─MaxPool1d: 1-6                         [1, 256, 1]               --\n",
              "├─LSTM: 1-7                              [1, 1, 512]               4,206,592\n",
              "├─Linear: 1-8                            [1, 128]                  65,664\n",
              "├─ReLU: 1-9                              [1, 128]                  --\n",
              "├─Dropout: 1-10                          [1, 128]                  --\n",
              "├─Linear: 1-11                           [1, 64]                   8,256\n",
              "├─ReLU: 1-12                             [1, 64]                   --\n",
              "├─Linear: 1-13                           [1, 1628]                 105,820\n",
              "├─Softmax: 1-14                          [1, 1628]                 --\n",
              "==========================================================================================\n",
              "Total params: 4,742,684\n",
              "Trainable params: 4,742,684\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 5.04\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.03\n",
              "Params size (MB): 18.97\n",
              "Estimated Total Size (MB): 19.00\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history2 = train(model2,\n",
        "                train_loader,\n",
        "                valid_loader,\n",
        "                model2_optimizer,\n",
        "                model2_criterion,\n",
        "                epochs=100\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klMgU6ojrAsm",
        "outputId": "a51eed73-fc5b-4446-b7aa-50a29a352efd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100 - Train loss 7.379 - Train accuracy 0.019 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 2/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 3/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 4/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 5/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 6/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 7/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 8/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 9/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 10/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 11/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 12/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 13/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 14/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 15/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 16/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 17/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 18/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 19/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 20/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 21/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 22/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 23/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 24/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 25/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 26/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 27/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 28/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 29/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 30/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 31/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 32/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 33/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 34/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 35/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 36/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 37/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 38/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 39/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 40/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 41/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 42/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 43/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 44/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 45/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 46/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 47/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 48/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 49/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 50/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 51/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 52/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 53/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 54/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 55/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 56/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 57/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 58/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 59/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 60/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 61/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 62/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 63/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 64/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 65/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 66/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 67/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 68/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 69/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 70/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 71/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 72/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 73/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 74/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 75/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 76/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 77/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 78/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 79/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 80/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 81/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 82/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 83/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 84/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 85/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 86/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 87/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 88/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 89/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 90/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 91/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 92/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 93/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 94/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 95/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 96/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 97/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 98/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 99/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 100/100 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Model3(nn.Module):\n",
        "    def __init__(self, vocab_size, output_dim):\n",
        "        super().__init__()\n",
        "        # Aumentamos el tamaño de embedding y las capas LSTM\n",
        "        self.embedding_dim = 128\n",
        "        self.lstm_size = 256\n",
        "        self.num_layers = 4  # Aumentamos el número de capas LSTM\n",
        "\n",
        "        # Embedding Layer\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size + 1, embedding_dim=self.embedding_dim, padding_idx=0)\n",
        "\n",
        "        # LSTM Layer\n",
        "        self.lstm1 = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
        "                             num_layers=self.num_layers, dropout=0.3)\n",
        "\n",
        "        # Fully Connected Layers con más neuronas\n",
        "        self.fc1 = nn.Linear(in_features=self.lstm_size, out_features=128)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
        "        self.fc3 = nn.Linear(in_features=64, out_features=32)\n",
        "        self.fc4 = nn.Linear(in_features=32, out_features=output_dim)\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x, prev_state=None):\n",
        "        if prev_state is None:\n",
        "            batch_size = x.shape[0]\n",
        "            prev_state = self.init_hidden(batch_size)\n",
        "\n",
        "        out = self.embedding(x)\n",
        "        lstm_output, (ht, ct) = self.lstm1(out, prev_state)\n",
        "\n",
        "        # Fully Connected layers\n",
        "        out = self.relu(self.fc1(lstm_output[:, -1, :]))  # Último hidden state\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(self.fc2(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(self.fc3(out))\n",
        "        out = self.fc4(out)\n",
        "        out = self.softmax(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.lstm_size),\n",
        "                torch.zeros(self.num_layers, batch_size, self.lstm_size))\n",
        "\n",
        "model3 = Model3(vocab_size=vocab_size, output_dim=output_dim)\n",
        "\n",
        "# Crear el optimizador y la función de error\n",
        "model3_optimizer = torch.optim.Adam(model3.parameters(), lr=0.001)\n",
        "model3_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Resumen del modelo\n",
        "summary(model3, input_size=(1, input_size), dtypes=['torch.IntTensor'], device=torch.device('cpu'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7jnbKcqWLmA",
        "outputId": "655404b0-7b39-4300-8e12-6dc8aedac228"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Model3                                   [1, 1628]                 --\n",
              "├─Embedding: 1-1                         [1, 3, 128]               208,512\n",
              "├─LSTM: 1-2                              [1, 3, 256]               1,974,272\n",
              "├─Linear: 1-3                            [1, 128]                  32,896\n",
              "├─ReLU: 1-4                              [1, 128]                  --\n",
              "├─Dropout: 1-5                           [1, 128]                  --\n",
              "├─Linear: 1-6                            [1, 64]                   8,256\n",
              "├─ReLU: 1-7                              [1, 64]                   --\n",
              "├─Dropout: 1-8                           [1, 64]                   --\n",
              "├─Linear: 1-9                            [1, 32]                   2,080\n",
              "├─ReLU: 1-10                             [1, 32]                   --\n",
              "├─Linear: 1-11                           [1, 1628]                 53,724\n",
              "├─Softmax: 1-12                          [1, 1628]                 --\n",
              "==========================================================================================\n",
              "Total params: 2,279,740\n",
              "Trainable params: 2,279,740\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 6.23\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.02\n",
              "Params size (MB): 9.12\n",
              "Estimated Total Size (MB): 9.14\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history3 = train(model3,\n",
        "                train_loader,\n",
        "                valid_loader,\n",
        "                model3_optimizer,\n",
        "                model3_criterion,\n",
        "                epochs=200\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY_RPZpzWViI",
        "outputId": "e9041252-9964-4414-eacb-f9aa07ea4cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/200 - Train loss 7.354 - Train accuracy 0.043 - Valid Loss 7.356 - Valid accuracy 0.041\n",
            "Epoch: 2/200 - Train loss 7.347 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 3/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 4/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 5/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 6/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 7/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 8/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 9/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 10/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 11/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 12/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 13/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 14/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 15/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 16/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 17/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 18/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 19/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 20/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 21/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 22/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 23/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 24/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 25/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 26/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 27/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 28/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 29/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 30/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 31/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 32/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 33/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 34/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 35/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 36/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 37/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 38/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 39/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 40/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 41/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 42/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 43/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 44/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 45/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 46/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 47/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 48/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 49/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 50/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 51/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 52/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 53/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 54/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 55/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 56/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 57/200 - Train loss 7.348 - Train accuracy 0.048 - Valid Loss 7.359 - Valid accuracy 0.037\n",
            "Epoch: 58/200 - Train loss 7.361 - Train accuracy 0.036 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 59/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 60/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.357 - Valid accuracy 0.040\n",
            "Epoch: 61/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 62/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 63/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 64/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 65/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 66/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 67/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 68/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 69/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 70/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 71/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 72/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 73/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 74/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 75/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 76/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 77/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 78/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 79/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 80/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 81/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 82/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 83/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 84/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 85/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 86/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 87/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 88/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 89/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 90/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 91/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 92/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 93/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 94/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 95/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 96/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 97/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 98/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 99/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 100/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 101/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 102/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 103/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 104/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 105/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 106/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 107/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 108/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 109/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 110/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 111/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 112/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 113/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 114/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 115/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 116/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 117/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 118/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 119/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 120/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 121/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 122/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 123/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 124/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 125/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 126/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 127/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 128/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 129/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 130/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 131/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 132/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 133/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 134/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 135/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 136/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 137/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 138/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 139/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 140/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 141/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 142/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 143/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 144/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 145/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 146/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 147/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 148/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 149/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 150/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 151/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 152/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 153/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 154/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 155/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 156/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 157/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 158/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 159/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 160/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 161/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 162/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 163/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 164/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 165/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 166/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 167/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 168/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 169/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 170/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 171/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 172/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 173/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 174/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 175/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 176/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 177/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 178/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 179/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 180/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 181/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 182/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 183/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 184/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 185/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 186/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 187/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 188/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 189/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 190/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 191/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 192/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n",
            "Epoch: 193/200 - Train loss 7.346 - Train accuracy 0.050 - Valid Loss 7.356 - Valid accuracy 0.040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = range(1, len(history1['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history1['accuracy'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=history1['val_accuracy'], label='valid')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QH6al4DoyUWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = range(1, len(history2['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history2['accuracy'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=history2['val_accuracy'], label='valid')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YPymBIIWgSJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Es importante__ destacar que en este ejemplo estamos entrenando nuestro propios Embeddings y para ello se requiere mucha data. En los ejemplos que realizaremos de aquí en más utilizaremos más datos, embeddings pre-enternados o modelos pre-entrenados."
      ],
      "metadata": {
        "id": "KoQLsiPu09SZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN6Fg_BsxJe6"
      },
      "source": [
        "### 5 - Predicción de próxima palabra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy_AXWQWzeeE"
      },
      "source": [
        "# pad_sequences\n",
        "# Si la secuencia de entrada supera al input_seq_len (3) se trunca\n",
        "# Si la secuencia es más corta se agregna ceros al comienzo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBvKHFPmzpy2"
      },
      "source": [
        "# Se utilizará gradio para ensayar el modelo\n",
        "# Herramienta poderosa para crear interfaces rápidas para ensayar modelos\n",
        "# https://gradio.app/\n",
        "import sys\n",
        "!{sys.executable} -m pip install gradio --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNyBykvhzs7-"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "def model_response(human_text):\n",
        "\n",
        "    # Encodeamos\n",
        "    encoded = tok.texts_to_sequences([human_text])[0]\n",
        "    # Si tienen distinto largo\n",
        "    encoded = pad_sequences([encoded], maxlen=3, padding='pre')\n",
        "\n",
        "    # Transformo a tensor\n",
        "    tensor = torch.from_numpy(encoded.astype(np.int32))\n",
        "\n",
        "    # Predicción softmax\n",
        "    y_hat = model1(tensor).argmax(axis=-1)\n",
        "\n",
        "    # Debemos buscar en el vocabulario la palabra\n",
        "    # que corresopnde al indice (y_hat) predicho por le modelo\n",
        "    out_word = ''\n",
        "    for word, index in tok.word_index.items():\n",
        "        if index == y_hat:\n",
        "            out_word = word\n",
        "            break\n",
        "\n",
        "    # Agrego la palabra a la frase predicha\n",
        "    return human_text + ' ' + out_word\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=model_response,\n",
        "    inputs=[\"textbox\"],\n",
        "    outputs=\"text\",\n",
        "    layout=\"vertical\")\n",
        "\n",
        "iface.launch(debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCeMWWupxN1-"
      },
      "source": [
        "### 6 - Generación de secuencias nuevas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwbS_pfhxvB3"
      },
      "source": [
        "def generate_seq(model, tokenizer, seed_text, max_length, n_words):\n",
        "    \"\"\"\n",
        "        Exec model sequence prediction\n",
        "\n",
        "        Args:\n",
        "            model (keras): modelo entrenado\n",
        "            tokenizer (keras tokenizer): tonenizer utilizado en el preprocesamiento\n",
        "            seed_text (string): texto de entrada (input_seq)\n",
        "            max_length (int): máxima longitud de la sequencia de entrada\n",
        "            n_words (int): números de palabras a agregar a la sequencia de entrada\n",
        "        returns:\n",
        "            output_text (string): sentencia con las \"n_words\" agregadas\n",
        "    \"\"\"\n",
        "    output_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "\t\t# Encodeamos\n",
        "        encoded = tokenizer.texts_to_sequences([output_text])[0]\n",
        "\t\t# Si tienen distinto largo\n",
        "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "        # Transformo a tensor\n",
        "        tensor = torch.from_numpy(encoded.astype(np.int32))\n",
        "\n",
        "        # Predicción softmax\n",
        "        y_hat = model1(tensor).argmax(axis=-1)\n",
        "\n",
        "\t\t# Vamos concatenando las predicciones\n",
        "        out_word = ''\n",
        "\n",
        "        # Debemos buscar en el vocabulario la palabra\n",
        "        # que corresopnde al indice (y_hat) predicho por le modelo\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == y_hat:\n",
        "                out_word = word\n",
        "                break\n",
        "\n",
        "\t\t# Agrego las palabras a la frase predicha\n",
        "        output_text += ' ' + out_word\n",
        "    return output_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoFqRC5pxzqS"
      },
      "source": [
        "input_text='hey jude don\\'t'\n",
        "\n",
        "generate_seq(model1, tok, input_text, max_length=3, n_words=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2SHmXbgxQH9"
      },
      "source": [
        "### 7 - Conclusiones\n",
        "El modelo entrenado tuvo un muy mail desempeño en el entrenamiento además de overfitting. Cuestiones que podrían mejorarse:\n",
        "- Agregar más capas o neuronaes\n",
        "- Incrementar la cantidad de épocas\n",
        "- Agregar BRNN\n",
        "\n",
        "Es importante destacar que en este ejemplo estamos entrenando nuestro propios Embeddings, y para ello se requiere mucha data. En los ejemplos que realizaremos de aquí en más utilizaremos más datos, embeddings pre-enternados o modelos pre-entrenados."
      ]
    }
  ]
}